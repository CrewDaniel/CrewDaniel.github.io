[ { "title": "[CS] 비동기 프로그래밍 (3) - 동시성 프로그래밍", "url": "/posts/cs_3/", "categories": "CS, Asynchronous", "tags": "CS, Async", "date": "2022-07-08 03:40:00 +0900", "snippet": "Android에서 애플리케이션을 실행하여 생성된 프로세스는 메인 스레드를 가지게 됩니다. 그리고 일반적으로 이 스레드는 UI 스레드로서, Android UI와 관련된 구성 요소(android.view, android.widget 패키지)가 작동하는 스레드입니다. 즉 이 스레드에서 UI의 상태를 변경하고, 사용자의 입력과 같은 이벤트를 받습니다. 따라서 이 스레드가 Blocking; 대기 상태에 놓이면 UI가 업데이트 되지 않으며, 이벤트에 반응할 수 없기 때문에 사용자는 앱을 사용할 수 없게 되며 에러가 발생한 것으로 여기게 됩니다. UI 스레드가 5초 이상 차단되면 ANR(Application Not Responding) 대화상자가 표시됩니다.따라서 Android 애플리케이션 개발에 있어서 네트워크, I/O 작업시에 가장 주의해야 할 것은 UI 스레드가 Blocking 되지 않도록 해야 한다는 것입니다. 이러한 작업들은 UI 스레드가 아닌 백그라운드 스레드에서 수행 되어야 합니다.이와 반대로 UI 상태를 업데이트하거나, 이벤트에 반응하는 작업은 UI 스레드에서만 수행되어야 합니다. 백그라운드 스레드에서 UI의 상태를 업데이트하거나, 이벤트에 반응하는 작업을 수행할 수 없습니다. 하지만 Android 프레임워크에서 제공하는 몇몇 함수(Activity.runOnUiThread(Runnable), View.post(Runnable) 등)를 통해 백그라운드 스레드에서 UI 스레드에 접근할 수 있습니다.이렇게 애플리케이션 개발에 여러 스레드를 사용하는 멀티스레드 프로그래밍과 I/O, 네트워크 작업, 연산이 오래걸리는 작업 등에 쓰이는 비동기 프로그래밍은 일반적인 개발에 비해 예외 처리나 함수 호출 시점에 있어 고려해야 할 사항이 더 많기 때문에 난이도가 높습니다.멀티스레드, 비동기 프로그래밍에서 생길 수 있는 문제들을 살펴보고, 이들에 대한 해결책에 대해 살펴보겠습니다.동시성 프로그래밍동시성 프로그래밍으로 작성되지 않은 코드를 우선 생각해봅시다. 우리가 일반적으로 쉽게 떠올릴 수 있는 코드 대부분이 해당될텐데, 코드가 작성된 순서대로 실행됩니다. 하나의 스레드(수행 흐름)가 존재하는 것입니다.동시성 프로그래밍은 여러 스레드(수행 흐름)가 존재하는 것입니다.동시성과 병렬성동시라는 표현은 처음 동시성 프로그래밍을 접하는 사람들에게 혼란을 줄 수 있다고 생각합니다. 동시라는 표현으로 인해 동시성 프로그래밍은 여러 수행 흐름(스레드)이 동시에 실행되는 것으로 인식됩니다.물론 애플리케이션의 사용자 경험에는 차이가 없을 수 있지만 실제로 실행되는, CPU 자원을 점유하는 것은 동시가 아닐 수 있습니다. 특히 멀티코어 프로세서가 기본이 된 현재에서는 병렬성과 동시성의 차이점을 명확히 하는 것이 이해에 도움이 될 것이라고 생각합니다.동시성동시성은 두 개 이상의 스레드의 실행 시간이 겹쳐질 때 발생합니다.하지만 이것이 두 개 이상의 스레드가 동시에, 병렬적으로 실행되고 있다는 의미는 아닙니다. CPU 스케줄러에서는 이 스레드들을 번갈아 가면서 할당하지만, 여러 스레드의 실행이 마쳐지지 않고 번갈아 가면서 동시에 실행되고 있다는 의미입니다.병렬성병렬성은 두 개 이상의 스레드가 같은 시간에 실행 중인 것을 의미합니다. 같은 시간에 하나의 CPU 코어에서는 하나의 스레드만 실행 가능합니다. 따라서 둘 이상의 스레드가 같은 시간에 실행 중이려면 적어도 두 개 이상의 CPU 코어가 존재해야(또는 분산 컴퓨팅) 할 것입니다.병렬성을 가진다면 동시성을 가진다고 할 수 있지만, 동시성을 가진다고 해서 병렬성을 가진 것은 아닙니다.그렇다면 병렬성이 동시성보다 항상 더 좋은, 상위 호환의 개념이 아닐까? 라고 생각할 수 있을 것입니다. 하지만 CPU 자원을 거의 필요로 하지 않는 작업이라면 여러 CPU 코어를 점유할 필요가 없을 것입니다.따라서 스레드에서 실행하는 작업이 CPU의 자원을 계속 필요로 하는 작업인지, 아니면 I/O나 네트워크와 같은 외부 입출력 작업인지에 따라 나누어 생각하고, 수행 방법도 다르게 한다면 더 효율적일 것입니다.CPU 바운드CPU 바운드는 CPU의 자원을 주로 필요로 하는 작업입니다.하지만 필요 이상으로 스레드를 생성하는 것 또한 오히려 성능을 저하시킬 수 있습니다. 예를 들어 코어가 하나인데 스레드가 10개일 경우, 각 스레드가 번갈아 가면서 실행될 것이고, 스레드의 생성, 실행되는 스레드의 교체 시에는 문맥 교환(Context Switching)을 위한 시간이 소요됩니다. 따라서 이 경우 오히려 스레드가 하나만 존재하고 하나의 코어에서 계속 실행되는 것이 오히려 좋은 방법일 수 있습니다.물리적인 코어와 소프트웨어의 스레드의 실행 관계를 고려하였을 때, 멀티코어 프로세서에서 CPU 자원 활용도를 높이기 위해서는 코어의 수만큼 스레드가 존재하는 것이 일반적으로 적절할 것입니다.I/O 바운드I/O 바운드는 CPU의 자원은 거의 필요로 하지 않고 대부분의 시간을 외부의 입력을 기다리는 것에 사용하는 작업입니다.즉, CPU 바운드의 작업에서는 멀티코어 프로세서의 성능을 최대한으로 활용하기 위해 여러 개의 스레드를 생성하고 작업을 나누어 수행하는 것이 중요하지만, I/O 바운드 작업은 CPU의 자원보다는 외부의 처리 결과를 반환받는 것이 중요하기 때문에 전체 실행 시간에 비해 CPU 자원을 필요로 하는 시간은 적습니다.지난 포스트에서 이런 경우 해당 프로세스/스레드를 Blocking 상태로 만들고, 스케줄러에서 다른 프로세스/스레드를 CPU에 할당한다는 것을 알 수 있었습니다. 하지만 Blocking 상태에서는 결과를 반환받기 전까지 다시 CPU 자원을 할당받을 수 없기 때문에, 해당 프로세스/스레드를 사용할 수 없게 됩니다. 또한 간단한 I/O나, 많은 I/O 작업을 동시에 수행할 때 이를 모두 각각의 스레드로 나눈다면, 각 스레드의 전환 과정에서 문맥 교환으로 인한 오버헤드 또한 성능을 낮추는 요인이 될 것입니다.SMP에서 발생할 수 있는 문제앞선 포스트들에서도 대칭형 다중 프로세서(Symmetric Multi-Processor, SMP)에 대한 이야기를 다루었습니다. 현재 Android OS를 사용하는 모바일 기기들의 거의 대부분이 SMP 모델을 따르고 있기 때문입니다. 각 CPU가 메모리의 접근에 있어서 동등한 권한을 가지고 있다는 의미는 한 CPU가 접근한 메모리 주소에 대해 다른 CPU도 똑같이 접근할 수 있다는 의미입니다.그렇기 때문에 SMP에서 작동하는 멀티스레드 프로그래밍에서는 여러 스레드가 각기 다른 CPU에서 동시에 접근할 수 있다는 가정을 하고 접근해야 아래에서 설명할 문제들을 방지할 수 있습니다.이하 내용은 프로세스 대신 스레드로 작성되었는데, 실제로는 프로세스와 스레드 모두 가능합니다. 다만 Android 애플리케이션 개발에 있어서는 스레드 간의 경합 상태가 발생할 가능성이 높기 때문에 스레드로 표현하였습니다.경합 상태경합 상태(Race Condition)는 두 개 이상의 스레드에서 같은 데이터에 동시에 접근하고, 이 중 하나 이상의 스레드에서 데이터를 수정할 때 발생할 수 있는 현상입니다.SMP에서는 메모리 영역을 모든 CPU 코어에서 접근할 수 있기 때문에(공유 자원) 서로 다른 스레드에서 같은 데이터에 접근할 수 있고, 동시성 프로그래밍에서는 이 순서를 예측하기 어렵습니다. 프로세스의 실행 결과는 일반적으로 항상 같아야 하지만 이렇게 실행 순서를 예측할 수 없게 되면 예측할 수 없는 상황이 발생할 수 있습니다.예를 들어 var num = 10 변수에 접근해 10을 더하는 스레드가 두 개 존재하고, 각자 한 번씩 실행한다고 가정하면 일반적으로 10을 두 번 더해 num = 30이 되는 것을 기대할 수 있을 것입니다. 하지만 이는 각 스레드가 순차적으로 실행되었을 때에는 정상적으로 작동하지만, 동시에 실행된다면 두 스레드가 각각 num = 10을 확인하고 10을 더해 num = 20으로 값을 쓰게 될 수 있습니다.또다른 예로 I/O 작업을 위해 공유 자원을 초기화 후 값을 가져온다고 가정했을 때, 작업이 미쳐 끝나지 않았음에도 불구하고 값을 다른 스레드에서 사용하는 경우 I/O 작업 스레드와 값을 사용하는 스레드의 실행의 실행 순서에 따라 성공 여부가 결정됩니다. 만약 I/O 작업 완료 이전에 값을 사용하는 스레드가 공유 자원에 접근한다면 에러가 발생할 것입니다.이렇게 공유 자원에 접근하는 순서에 따라 다른 결과를 낼 수 있는 영역을 임계 구역(Critical Section) 이라고 합니다. 임계 구역에서 발생할 수 있는 문제를 해결하기 위해서는 동시성 프로그래밍이라도 결과를 보장하도록 일부 과정에 있어 순차적으로 작업이 진행될 필요가 있습니다.교착 상태교착 상태(Deadlock)는 프로세스/스레드의 자원 할당, 임계 구역 접근 시에 발생할 수 있습니다. 특정 프로세스/스레드가 자원을 점유한 상태로 이를 해제하지 않고, 다른 프로세스/스레드가 이 자원을 필요로 한다면, 점유한 자원을 해제해야 할 것입니다. 만약 두 프로세스/스레드가 각각 두 개의 자원을 필요로 하는데 하나씩만 가지고 있다면, 어떤 프로세스/스레드는 가지고 있던 자원을 양보하여 먼저 작업을 끝내게 할 수 있을 것입니다. 하지만 그렇지 못하고 서로 상대의 자원 해제만을 기다린다면 더 이상 작업을 진행할 수 없을 것입니다.이러한 교착 상태가 발생하기 위해서는 아래의 네 가지 조건을 모두 만족해야 합니다.발생 조건상호 배제: 경합 상태를 해결하기 위한 방법이 오히려 교착 상태의 조건이 될 수 있기 때문에 락의 사용에 있어서는 주의가 필요합니다.비선점: 특정 프로세스/스레드가 가진 자원을 다른 프로세스/스레드가 빼앗을 수 없습니다. 스스로 자원을 반납할 때까지 기다려야만 합니다.점유 상태로 대기: 프로세스/스레드가 자원을 점유한 상태에서 다른 자원을 기다리고 있는 상황입니다. 이 때 이 프로세스/스레드가 점유한 자원은 다른 프로세스/스레드가 필요로 하는 자원일 수 있습니다.원형 대기: 점유 상태로 대기하는 프로세스/스레드가 원형을 이루어야 합니다. 이 때 원형이라는 것은 서로 필요로 하는 자원과 보유를 하는 자원의 관계입니다.해결법위의 네 가지 중 하나만 해결되어도 교착 상태를 해결할 수 있기 때문에 하나씩 살펴보겠습니다.우선 상호 배제의 경우는 락을 쓰지 않는다면 해결할 수 있을 것입니다. 하지만 락을 쓰는 경우는 대부분 경합 상태와 같이 문제가 발생할 수 있는 상황을 해결하기 위한 것이기 때문에 이 방법은 거의 쓰이지 않을 것입니다.비선점의 경우 선점으로 바꾼다면 해결할 수 있을 것입니다. 선점형 CPU 스케줄링을 사용하거나, 스레드의 경우 우선 순위를 지원하는 프로그래밍 언어를 사용한다면 해결될 것입니다.점유 상태로 대기의 경우 자원을 순차적으로 획득하는 것이 아닌, 모든 자원을 동시에 획득하도록 하는 방식을 사용한다면 해결될 수 있을 것입니다. 또는 락을 획득하는 순서에 있어 코드를 수정하여 해결할 수 있을 것입니다.원형 대기의 경우에도 우선 순위를 가지게 하여 순환하지 않고 먼저 자원을 획득할 프로세스/스레드를 지정한다면 해결할 수 있을 것입니다.해결을 위한 동기화 기법들임계 구역에서 발생할 수 있는 문제들을 해결 하기 위해서는 세 가지 조건을 모두 만족해야 합니다.상호 배제: 한 스레드가 임계 구역에 접근하면 다른 스레드는 접근할 수 없어야 합니다.한정 대기: 스레드가 임계 구역 접근을 위해 무한정 기다려서는 안됩니다.진행의 융통성: 한 스레드가 임계 구역에 접근하는 중이 아니라면 다른 스레드가 진행에 방해받지 않고 임계 구역에 접근할 수 있어야 합니다.앞서 살펴본대로 임계 구역에서는 순차적으로 접근할 수 있도록 제한을 할 필요가 있습니다. 이를 위해서 임계 구역에 락을 설정할 수 있습니다. 특정 스레드가 임계 구역의 데이터를 사용하면 락을 건 후, 사용을 마칠 때 락을 다시 풀어 다른 스레드에서 접근할 수 있도록 하는 것입니다. 위의 세 가지 조건을 모두 만족할 수 있도록 락을 적절히 구현하여야 합니다.이러한 락 기법을 Mutex(뮤텍스)라고 부르기도 합니다. 상호 배제(Mutual Exclusion)의 줄임말로, 위 세 가지 조건을 만족하기 위해 다양한 알고리즘이 고안되었습니다. 그 중에서 많이 쓰이는 기법에 대해 정리해보겠습니다.뮤텍스이를 구현하는 가장 간단한 방법으로 임계 구역에 접근하는 스레드가 Boolean 등의 변수를 통해 접근을 알린 후, 사용이 끝나면 다시 Boolean 값을 원래대로 돌려놓도록 하는 것입니다. 이 때 다른 스레드가 임계 구역에 접근할 때에는 먼저 Boolean 값을 확인하여 다른 스레드가 접근하는 중인지 알 수 있을 것입니다. 즉 Boolean 변수가 락의 역할을 하는 것입니다. 이해하기도 간단하고 구현하기도 쉽지만, 많은 문제점을 가지고 있습니다.우선 경합 상태에서 예시로 들었던 동시 접근 문제가 여전히 발생할 수 있습니다. 한 스레드가 Boolean 값을 확인 후 실행은 했지만, 아직 Boolean 값을 바꾸지 못했는데 스케줄러에 의해 타임아웃이 발생해 다시 큐로 돌아가고, 다른 스레드가 Boolean 값을 확인하였을 때에는 락이 잠기지 않은 상태일 것입니다.이를 개선하기 위한 다양한 알고리즘들이 나왔지만, 현재 많이 사용되고 있는 방법 중 대표적인 방법은 아래의 세마포어와 모니터입니다.세마포어Semaphore(세마포어)는 임계 구역에 진입할 수 있는 스레드의 수를 제한합니다. 뮤텍스와의 차이점은 뮤텍스를 구현하기 위한 다양한 알고리즘이 제시되었는데, 데이크스트라 알고리즘으로 기억에 남는 에츠허르 데이크스트라가 고안한 이 알고리즘은 다른 방식에 비해 간단합니다.임계구역에 접근하려는 스레드는 wait()을 수행하여 현재 임계 구역에 접근할 수 있는지 세마포어에 묻습니다. 세마포어는 한 번에 접근 가능한 스레드의 수에 기반한 현재 접근 가능한 스레드의 카운터를 가지고 있어 접근이 가능하면 카운터를 하나 빼고 임계 구역에 접근합니다. 이미 카운터가 0이라면 세마포어는 임계 구역을 사용하려는 스레드의 큐에 스레드를 등록합니다. 임계 구역에 접근했던 스레드가 작업을 마치면 signal() 신호를 보냅니다. 그러면 세마포어에서는 저장되어 있던 대기 큐 중 하나를 빼서 임계 구역에 접근할 수 있도록 합니다.세마포어에서 임계 구역에 접근하려는 스레드의 수를 1로 제한한다면, 뮤텍스처럼 사용할 수 있습니다.Java의 Semaphore 클래스는 acquire()와 release()가 각각 wait()와 signal()의 역할을 합니다.하지만 세마포어에서도 여전히 문제를 가지고 있습니다. 각 스레드가 wait() 와 signal()을 적절한 때에 반드시 호출해주어야 하며, 순서가 바뀌거나 둘 중 하나라도 빠지면 세마포어 전체에 영향을 끼치게 될 것입니다. 카운터가 제대로 작동하지 않거나 계속 임계 구역을 점유하게 되어 여전히 교착 상태의 위험성을 가지고 있습니다.모니터모니터는 세마포어를 구현하면서 발생할 수 있는 에러의 가능성을 줄이기 위해 임계 구역을 직접적으로 스레드에서 접근하는 것 대신 스레드에 접근을 위한 인터페이스만 제공합니다.다른 알고리즘에서 발생할 수 있는 문제에 대해 해결책들을 개선하였고 안전성도 높기 때문에 많은 프로그래밍 언어에서 사용되고 있습니다. 대표적으로 Java의 동기화 기능들인 ReentrantLock, synchronized 키워드 등은 모니터를 사용합니다.정리과거 Android 프레임워크에서는 비동기 프로그래밍을 돕는 AsyncTask 라는 클래스가 존재했지만 현재는 deprecated 되었습니다. 대신 Java의 java.util.concurrent의 API들을 사용하거나 Kotlin의 Coroutines 사용을 안내하고 있습니다.Kotlin의 Coroutines는 Java의 java.util.concurrent의 기능들에 더해 스레드 상태를 Non-Blocking으로 유지하는 방법(일시 정지)에 대해 다양하면서 쉬운 해결책을 제시합니다. 또한 스레드의 실행 흐름을 더 작게 나누어 한 스레드에서 여러 실행 흐름을 유지할 수 있는 코루틴(한 스레드 내에서 동시성을 가지도록 수행 흐름을 나눔)을 지원합니다. 이에 대해서 다음 포스트에 다루도록 하겠습니다.참고자료 코틀린 동시성 프로그래밍(미구엘 엔젤 카스티블랑코 토레스) 쉽게 배우는 운영체제(조성호) Android용 SMP 기본 지침서 프로세스 및 스레드 개요 Shared mutable state and concurrency" }, { "title": "[CS] 비동기 프로그래밍 (2) - 리눅스의 프로세스, 스레드, 태스크", "url": "/posts/cs_2/", "categories": "CS, Asynchronous", "tags": "CS, OS, Linux", "date": "2022-06-28 03:30:00 +0900", "snippet": "Android의 스레드 사용 방법을 알기 전에, 리눅스의 스레드와 프로세스는 어떤 원리로 작동되는 것인지, 코어가 여러 개인 CPU에서 실제로 여러 프로세스나 스레드가 병렬적으로 작동하는지 알고 싶어 이 포스트로 정리하게 되었습니다. 운영체제라는 과목에서 일반적으로 배우는 프로세스, 스레드의 개념과 리눅스에서의 프로세스, 스레드는 유사하지만 약간의 차이가 있습니다. 이 차이를 몰랐을 때 저는 왜 안드로이드 앱 개발에서 스레드 대신에 Coroutine 사용을 권장하는 것인지 의문이 있었습니다. 그냥 더 가볍고, 스레드를 적게 생성한다는 설명만으로는 이해하기 어려운 부분이 있습니다. 왜냐하면 운영체제 과목에서는 스레드를 일종의 경량 프로세스이며, 프로세스의 수행 흐름을 여러 개로 나눌 수 있다고 배웠는데 왜 경량 프로세스보다 더 가벼운 것이 필요하게 되었을지에 대한 설명은 찾을 수 없었기 때문입니다.리눅스 커널을 사용하는 안드로이드 OS이기 때문에, 리눅스 커널에서 프로세스와 스레드의 생성과 작동 방식을 알게 된다면 위의 내용에 대한 해답을 얻는 것에 도움이 될 것이라고 생각되어 정리하였습니다. 따라서 운영체제 과목에서 배우는 프로세스와 스레드에 대한 일반적인 내용은 생략하겠습니다.태스크리눅스 커널에서 프로세스와 스레드를 관리하는 단위는 태스크(Task)입니다. 프로세스와 스레드가 모두 같은 task_struct라는 자료구조로 생성되고 관리됩니다. 이 자료구조는 sched.h 파일에 정의되어 있으므로 프로세스, 스레드의 생성과 관리에 있어 POSIX API인 unistd.h와 함께 많이 쓰입니다.리눅스에서 프로세스 생성 시에는 fork()를 사용하고(이후 execl() 등으로 새로운 작업 수행), 스레드 생성 시에는 clone()이나 pthread_create() 함수를 사용합니다. 그런데 이렇게 여러 함수를 사용해도 커널 내부에서는 똑같이 do_fork() 함수를 실행하여 요청을 수행합니다.프로세스뿐만이 아닌 스레드가 생성될 때에도 똑같이 태스크를 생성합니다. 즉 하나의 스레드가 생성될 때마다 그에 대응되는 태스크가 존재한다는 의미입니다. 이러한 방식은 리눅스 커널 2.6에서 NPTL을 채택한 이후부터 적용되었습니다. 이전까지는 사용자가 생성한 스레드의 수 그대로 커널에서 스레드를 생성한다는 보장이 없는 n:1 모델이었지만, NPTL은 사용자가 스레드 생성시 커널에서도 그와 같은 수의 스레드를 생성하고 관리하는 1:1 모델입니다.이후에 다시 다루겠지만, 이런 관점에서 봤을 때 Coroutine의 필요성이 느껴집니다. 일시적인 작업 또는 간단한 작업을 위해 매번 새로운 스레드를 생성해서 사용한다면(물론 스레드 풀 등의 방법으로 새로운 스레드의 생성을 최대한 억제할 수도 있을 것입니다) 생성에도 시간이 필요할 것이고, 그 스레드는 제거되기 전까지 메모리를 점유할 것이며, 다른 작업으로 전환 시에 문맥 교환 작업에 시간이 소요되는 등 많은 오버헤드가 필요로 할 것입니다. 이 게시글의 첫 부분에서 적은 문제에 대해 답이 될 수 있는 부분이라고 생각됩니다. 우선 Coroutine에 대해서는 여기까지만 생각해보고, 태스크에 대해 마저 알아보도록 합시다.그렇다면 프로세스와 스레드가 같은 자료구조로 관리된다면, 어떻게 둘을 구분할 수 있을까요? 또, 특정 스레드가 어떤 프로세스에 속한 것인지는 어떻게 알 수 있을까요? 우선 생각할 수 있는 것은 PID를 같게 하는 것입니다. 같은 프로세스니 같은 프로세스 ID가 같아도 될 것이라고 생각됩니다. 하지만 그렇다면 같은 프로세스 내의 여러 스레드인 태스크들을 구분할 방법이 없을 것입니다. 리눅스에서는 이에 대한 해결방법으로 task_struct에서 같은 프로세스의 스레드 그룹이라는 것을 알 수 있는 TGID 라는 별도의 구분값을 가지도록 하고, PID는 같은 프로세스더라도 서로 다른 값을 가지도록 합니다.Copy-On-Write새로운 프로세스를 생성할 때, 일반적으로 fork()를 호출하여 부모 프로세스를 그대로 복사한 후 exec()을 호출하여 프로세스를 수정합니다. 하지만 fork()의 단점을 살펴보자면, 부모 프로세스 복제에 긴 시간이 걸리고, 새로 필요하게 될 프로세스의 크기와 관련 없이 부모 프로세스의 크기만큼 메모리 영역을 필요로 하게 된다는 것입니다.리눅스에서는 이를 개선하기 위한 방식을 도입합니다. Copy-On-Write는 새로운 태스크를 만들 때, PID 등의 일부 정보를 제외하고 부모 프로세스 태스크의 데이터를 그대로 가리키는 태스크를 만드는 기법입니다. 이렇게 하면 메모리 영역을 추가로 차지할 필요 없이, 같은 곳을 가리키기만 하면 됩니다. 그리고 새로운 값을 쓸 필요가 생기게 된다면 그 때 기존에 가리키던 영역은 그대로 두고, 새로운 메모리 영역에 값을 작성하고 그 주소를 가리키도록 하는 것입니다.SMP대칭형 다중 프로세서(Symmetric Multi-Processor, SMP)는 모든 CPU 코어가 대칭, 즉 동등하다는 의미입니다. 동등하다는 의미에는 각 CPU 코어가 메모리 접근에 있어서 동등한 권한을 가지고 있으며, 입출력 버스 또한 똑같이 공유합니다. 이와 대비되는 개념으로 불균일 기억 장치 접근(Non-Uniform Memory Access, NUMA)이 있지만, 여기서는 SMP에 집중하여 접근해보겠습니다.런 큐, 스케줄러태스크들은 CPU를 필요로 하지만, 태스크의 수에 비해 CPU 코어의 수는 매우 부족합니다. 여러 태스크가 각자 순서를 정해 돌아가면서 사용할 필요가 있을 것입니다. 실행 가능한 상태(준비 상태, 실행 상태. TASK_RUNNING)의 태스크들을 모아 적절한 순서대로 시키기 위해 리눅스에서는 런 큐 라고 하는 자료구조(rq)로 관리합니다. 이 자료구조의 구현은 위에서 task_struct의 자료구조가 정의된 sched.h 파일에 있습니다. 이러한 런 큐는 CPU의 각 코어마다 하나씩 가지게 되고, 실행 가능한 상태의 모든 태스크는 런 큐 중 한 곳에 속해 실행을 기다리게 됩니다.새롭게 태스크가 생성되었는데 런 큐가 여러 개라면, 이 태스크는 어떤 런 큐로 가는 것이 좋을까요? 일반적으로 부모 태스크가 속한 런 큐에 삽입됩니다. 그 이유는 제가 지난 포스트에서 캐시 메모리의 지역성 원리에 대해 작성했습니다. 부모 태스크와 공유하는 데이터가 있다면 데이터 지역성의 원리를 따르게 될 것이니 성능의 이점을 가질 수 있을 것입니다.각 런 큐가 모두 비슷한 작업량의 태스크를 가지고 있다면 좋겠지만, 시간이 지나면 각 런 큐마다 차이가 발생하게 될 것입니다. 이러한 부하에 있어서 차이가 발생하였을 때 이를 균등하게 재배치하는 부하 균등(load balancing) 기법을 사용, 런 큐 간에 태스크를 이동하여 부하를 조절합니다.각 런 큐에서는 스케줄러가 작동해 CPU 코어에서 수행할 태스크를 정합니다. 태스크를 고르는 스케줄링의 방법은 다양하지만 이 글에서는 간단하게 일반 태스크를 위한 기본 정책만 알아보겠습니다.CFS 스케줄러리눅스에서 일반 태스크 스케줄링에 쓰이는 기법을 CFS(Completely Fair Scheduler) 라고 합니다. 이름에서는 완전히 공평한 것을 추구하지만, 그렇다고 해서 모든 태스크가 완전히 같은 시간동안 작업을 수행해야 한다는 것은 아닙니다. 태스크의 우선순위에 가중치를 주어 우선순위가 높은 태스크는 CPU 할당시 조금 더 긴 시간 동안 사용할 수 있도록 해 태스크가 해야 할 일을 더 빨리 끝낼 수 있도록 합니다.이런 동작을 수행하기 위해 리눅스에서는 vruntime 이라는 개념을 도입하였습니다. 각 태스크는 vruntime을 가져 CPU를 사용한 시간과 우선순위에 따라 자신의 vruntime을 증가시킵니다. 우선순위가 높을수록(값이 낮을수록) vruntime의 증가 값이 작아지도록 합니다. 따라서 우선순위가 높으면 vruntime이 작게(느리게) 증가해 같은 vruntime을 채우기 위해서 더 긴 시간동안 CPU를 사용할 수 있는 것입니다.스케줄러가 태스크를 고를 때 vruntime이 가장 작은 태스크를 고르면 됩니다. vruntime이 가장 작은 태스크는 가장 과거에 CPU를 사용한 것을 의미하기도 합니다. 리눅스는 가장 작은 vruntime을 가진 태스크를 고르기 위해 vruntime을 키 값으로 하는 Red-Black Tree (RBTree) 자료구조를 사용합니다.여기서 한 가지 의문이 생깁니다. 힙 자료구조를 사용하면 최소값을 O(1)에 바로 찾을 수 있을텐데 왜 탐색에 O(log n)이 필요한 RBTree를 사용할까요? StackOverflow의 답변에 따르면, 힙 자료구조는 배열로 구현되어 있고, 따라서 연속적인 메모리 공간을 필요로 합니다. 수많은 태스크를 관리하려면 큰 배열, 즉 큰 연속적인 메모리 공간을 필요로 한다는 단점이 있기 때문입니다. 또한 힙에서 데이터의 추가, 삭제, 재정렬은 O(log n)이 필요하기 때문에 RBTree와 차이가 없고, 따라서 배열 의 문제점을 개선하는 포인터 기반의 힙을 만들어 쓰기 보다는 같은 성능을 보이는 RBTree를 쓰기로 한 것입니다.결론적으로 각 태스크는 OS에서 효율적으로 사용 시간을 배분해주니, 애플리케이션 개발자는 필요에 따라 스레드를 나누어서 멀티스레드 프로그래밍으로 성능을 최대한 끌어올려야 할 것입니다. 다음 글에서는 Kotlin에서의 멀티스레드 프로그래밍과 Coroutine에 대한 내용을 작성하겠습니다.참고자료 리눅스 커널 내부구조(백승재, 최종무) 쉽게 배우는 운영체제(조성호)" }, { "title": "[CS] 비동기 프로그래밍 (1) - 캐시 메모리", "url": "/posts/cs_1/", "categories": "CS, Asynchronous", "tags": "CS, Asynchronous", "date": "2022-02-19 23:00:00 +0900", "snippet": "비동기 프로그래밍에 대해 공부를 하다보니 굉장히 멀리 오게 되었고, 그래서 하나씩 정리를 하려고 합니다. bottom-up 방식으로 작성하여 첫 게시글인 캐시 메모리에서부터 가장 고단위인 코루틴까지 작성할 예정입니다.캐시 메모리캐시 메모리에 대해 모든 내용을 작성하기 보다는, 비동기 프로그래밍을 이해하는 것에 필요한 내용 위주로 정리하였습니다.데이터 지역성캐시 메모리는 데이터의 지역성을 이용하여 처리 속도를 높입니다. 캐시 메모리가 가져오는 데이터는 프로세서에서 자주 쓰이는 데이터여야 합니다. 한 번만 쓰고 다시 다른 데이터를 불러온다면, 그냥 저장소에서 바로 가져오는 것이 여러 중간단계를 거치는 것보다 더 빠를 것입니다. 캐시 메모리에 쓰이는 데이터는 프로세서에서 자주 접근하는 데이터로 구성해야 이득을 얻을 수 있을텐데, 그 데이터를 어떻게 알 수 있을까요?자주 사용하는 데이터는 데이터 지역성의 원리를 따릅니다. 시간 지역성과 공간 지역성으로 나누어서 볼 수 있습니다.시간 지역성은 최근 접근한 데이터에 다시 접근할 확률이 높다는 것을 의미합니다. 예를 들어 반복문에서 같은 변수에 여러 번 접근하는 경우가 있습니다.공간 지역성은 최근 접근한 데이터의 주변 데이터에 다시 접근할 확률이 높다는 것을 의미합니다. 배열은 연속적으로 할당되기에, 특정 배열의 데이터들을 접근한다면 공간적으로 연속된 데이터들에 접근하는 것을 의미합니다.for(i in 1..100) {arr[i] = i}위의 반복문에서 변수 i가 호출되는 것은 시간 지역성을 의미하며, arr[i]에서 배열의 원소를 연속적으로 호출하는 것은 공간 지역성을 의미합니다.캐시 메모리의 구조프로세서마다 차이가 있지만, 많은 프로세서에서 각 코어마다 L1 캐시와 L2 캐시를 가지고, 모든 코어가 공유하는 L3 캐시를 둡니다. 위 자료의 오른쪽 하단에서 L1과 L2에는 앞에 4X라고 표시된 것이 4개가 장착되었다는 것으로 4코어 프로세서인 i7-8650U의 각 코어에 할당된 것입니다. 그리고 L3 캐시는 별다른 표시가 없으므로 하나만 존재한다는 것을 알 수 있습니다.8-way set associative, 64-byte line size는 캐시 라인의 크기와, 캐시 매핑 정책에 대한 것입니다.프로세서가 한 사이클에 처리할 수 있는 단위인 워드 몇 개를 묶어 하나의 캐시 라인(캐시 블록)이라고 합니다. line size는 캐시 라인의 크기를 의미하며, 64byte는 64bit의 워드 8개가 묶인 것으로 이해할 수 있습니다. 캐시의 데이터를 교체할 때는 특정 워드 하나만 교체할 수 없으며, 캐시 라인 전체를 교체해야 합니다. 그 이유는 캐시 데이터 교체에는 비용이 드는데, 공간 지역성을 생각하면 인접한 데이터들도 사용될 확률이 높기 때문에 함께 가져오는 것으로 이해할 수 있습니다.주 메모리 등에서 캐시 라인을 가져와 캐시에 저장할 때 캐시 라인의 데이터를 식별하기 위한 태그 비트와 캐시 라인의 유효성을 검증하기 위한 유효 비트를 포함하여 하나의 캐시 엔트리가 완성됩니다.그렇다면 캐시 엔트리는 캐시 메모리 공간 중 어디에 배치되어야 할까요? 8-way set associative가 바로 그 내용을 담고 있습니다.캐시 배치 정책(출처: By Hellisp - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=851638)Direct Mapped Cache 방식에서는 캐시 메모리의 각 주소마다 저장될 메인 메모리의 주소가 정해져 있습니다. 캐시 메모리의 크기보다 메인 메모리의 크기가 훨씬 크기 때문에, 하나의 캐시 메모리 주소에 할당된 메인 메모리 주소가 많을 것입니다. 하지만 하나의 캐시 메모리 주소에 들어갈 수 있는 데이터는 하나의 메인 메모리 주소에 해당하는 데이터만 가능하기 때문에 같은 메모리 주소에 해당하는 메인 메모리의 주소를 여러 개 호출할 경우 바로 캐시 미스가 발생한다는 단점이 있습니다.이 방법은 이상적인 경우 겹치는 주소가 하나도 없이 모든 캐시 메모리 공간에 배치되지만, 최악의 경우에는 하나의 캐시 메모리 주소에 서로 다른 메인 메모리 주소들이 계속 갱신되어 계속 캐시 미스가 발생할 수 있을 것입니다. 이렇게 캐시 미스가 발생할 확률이 꽤 높기 때문에 현재 쓰이는 방식은 아닙니다.다음으로 Fully Associative Cache 방식이 있습니다. 이 방식은 비어있는 캐시 메모리 주소가 있으면 바로 저장합니다. 저장할 때도 쉬워보이고, 캐시 미스가 발생할 확률도 위의 Direct Mapped Cache 방식에 비하면 매우 적어보입니다. 하지만 캐시 메모리에서 필요한 데이터를 찾기 위해서는 일일히 검색해야 하기 때문에 시간이 오래 걸릴 것입니다. 따라서 이 방법도 거의 쓰이지 않습니다.그렇다면 이 두 방식을 적절히 섞는 것은 어떨까요? Set Associative Cache 방식은 Direct Mapped Cache 처럼 메인 메모리 주소에 해당하는 캐시 메모리 주소를 두되, 그 주소에 들어갈 수 있는 메인 메모리 주소를 하나가 아니라 여러 개를 두고 그 중 비어 있는 곳에 Fully Associative Cache 처럼 저장하는 방식입니다.8-way set associative는 특정 주소마다 8개의 캐시 엔트리를 저장하는 방식이라는 의미입니다.캐시 교체 정책캐시 메모리의 공간은 한정되어 있고 메인 메모리에 비하면 매우 작은 크기이기 때문에 빠른 시간에 모든 공간에 데이터가 차게 될 것입니다. 예를 들어 위에서 8-way set associative의 경우 8개의 캐시 엔트리를 모두 채운 이후에 새로운 데이터를 캐시에 저장하고 싶다면 기존의 데이터를 새 데이터로 교체해야 하는데, 어떤 데이터를 교체할 것인지에 대한 수많은 정책들이 있습니다.그 중에서 많이 쓰이는 방법은 LRU (Least Recently Used), 또는 이에 기반하여 변형된 정책입니다. 주어진 공간이 모두 채워졌을 경우, 가장 이전에 사용된 데이터를 교체하는 방식입니다. 시간 지역성을 고려해보면 최근에 쓰였던 데이터는 다시 쓰일 확률이 높기 때문에 가장 이전에 쓰인 데이터를 교체하는 것으로 이해할 수 있습니다.이 방식을 고수준 프로그래밍 언어로 구현한다면 Doubly Linked List와 HashMap을 이용하거나, Java와 같은 언어에서는 LinkedHashMap으로 구현할 수 있을 것입니다.SMPSMP(Symmetric Multi-Processor, 대칭형 다중 프로세서)고려해야 할 문제점들비동기 프로그래밍에서 고려해야 할 문제점들 중에서 위의 내용들과 관련된, 프로세서와 메모리 단계에서 생각해야 할 문제점들이 있습니다. 프로그래머는 아래의 문제점들을 고려하면서 코드를 작성해야 합니다.비순차적 명령어 처리우선 프로세서가 들어오는 명령들을 순서대로 실행하지 않는 것이 있습니다. 예를 들어 여러 명령어가 프로세서에 들어 왔을 때 특정 명령어에서 로드한 메모리의 캐시 미스가 발생한다면 다시 데이터를 찾아 교체하는 시간이 필요할 것입니다. 그 동안 프로세서가 아무런 일을 하지 않고 기다리는 대신에, 기다리는 데이터와 관련 없는 다른 명령어가 그 이후에 있다면 그 명령어를 먼저 처리합니다. 또한 멀티스레드가 아니더라도 동시에 처리할 수 있는 명령어들은 동시에 먼저 처리하는 등의 방법으로 프로세서의 효율을 높입니다. 이를 비순차적 명령어 처리라고 합니다.명령을 처리할 수 있는 코어가 하나인 프로세서이거나, 싱글 스레드로 작성된 프로그램이라면 위와 같이 비순차적으로 명령어를 처리해도 프로그래머 입장에서는 아무런 문제를 느낄 수 없습니다. 그 이유는 명령어 주소를 계산하여 그 주소에서 명령어를 가져오는 인출 단계와 명령어 완료 단계는 순차적으로 진행되기 때문에 결과적으로 작성된 코드가 순서상으로 변화되지 않았기 때문입니다.하지만 프로세서의 코어가 여러 개이고, 멀티 스레드로 작성된 프로그램이라면 경우가 복잡해집니다. 예를 들어 L3 캐시 또는 메인 메모리에서 데이터를 인출하여 레지스터의 큐에 값을 저장한 동안 다른 코어가 같은 메모리 주소의 데이터를 변경시켰을 수도 있습니다. 하나의 메모리에 접근하고 변경할 수 있는 코어가 여러 개이기 때문에 발생하는 현상인 것입니다. 또한 모바일 프로세서는 대부분 x86이 아닌 ARM인데 저장 명령의 순서가 유지되지 않고, 모든 코어에 저장 명령이 동시에 도달하지 않을 수도 있습니다. 또한 이렇게 순서가 바뀌는 것은 프로세서 뿐만이 아니라 성능 향상을 위해 컴파일러에 의해서도 발생할 수 있습니다.데이터 경합여러 스레드에서 같은 데이터에 동시에 접근하고, 데이터를 수정할 때 데이터 경합이 발생합니다.한 스레드에서 A = 5 로 값을 할당하고, 다른 스레드에서 이 값을 읽는다고 할 때, 어떤 스레드가 먼저 실행될지 알 수 없기 때문에 A의 값이 어떻게 관찰될지 보장할 수 없습니다.이 문제를 해결하기 위해서는 특정 변수에 대해 여러 스레드가 동시에 접근하는 것을 막거나 데이터의 원자성을 보장해야 하며, 많은 프로그래밍 언어에서 Mutex, Volatile 등의 다양한 방법으로 지원합니다.가짜 공유데이터 경합을 막기 위해 락을 사용했다고 가정하면, 락이 걸린 캐시 주소에 해당하는 캐시 라인을 무효화 시킵니다. 위에서 캐시 라인별로 태그 비트가 달린다는 것을 알았기 때문에 왜 캐시 라인 전체가 무효화 되는지는 이해할 수 있을 것입니다.그런데 여러 코어에서 동시에 같은 캐시 라인에 접근하려고 할 때 이렇게 락이 걸린다면, 안전하지만 성능 저하가 발생할 수 있을 것입니다. 무효화가 풀릴 때까지 다른 코어는 무효화가 풀리기를 기다려야 하기 때문입니다. 그렇다면 병렬적으로 작성된 코드도 실제로는 여러 스레드가 돌아가면서 작업하니 동시성은 만족하지만 병렬성은 달성하지 못하게 될 수도 있습니다.해결 방법들이러한 문제들을 해결하기 위해 프로그래밍 언어에서 다양한 기법들을 라이브러리로 제공하고 있습니다. 이에 대한 내용은 이후의 포스트에서 별도로 정리하겠습니다." }, { "title": "[Android] 동영상에 그림 그리기 (10) - 개선 리스트", "url": "/posts/video_memo_10/", "categories": "Android, boomerang", "tags": "Android, OpenGL ES, Shader", "date": "2022-01-24 03:00:00 +0900", "snippet": "이번 포스트에서는 boomerang 앱을 완성한 이후 수정, 개선한 부분들에 대해 작성하겠습니다. 이 포스트의 내용은 코드의 수정사항이 생기면 계속 추가하겠습니다.Vertex Buffer Object지난 포스트에서 Vertex Buffer에 대해 작성하면서, java.nio 패키지의 ByteBuffer를 사용하는 방법(Client-side buffer)과, OpenGL ES에서 버퍼를 생성하는 두 가지 방법이 있다고 하였습니다. OpenGL ES의 버퍼를 사용하는 방식으로 수정해보았습니다.val vertexBuffer = IntArray(1)GLES20.glGenBuffers(1, vertexBuffer, 0)우선 버퍼를 생성해야 합니다. IntArray를 생성하는 것은 빈 배열을 생성하고 그 곳에 생성한 OpenGL ES의 버퍼에 접근할 수 있는 값을 넣어주는 방식입니다. 이러한 방식은 EGL 초기화나 텍스처 생성 등에서 쓰였던 방식이니 이제 익숙하실겁니다.GLES20.glBindBuffer(GLES20.GL_ARRAY_BUFFER, vertexBuffer[0])GLES20.glBufferData(GLES20.GL_ARRAY_BUFFER, 32, vertexBufferOld, GLES20.GL_STATIC_DRAW)GLES20.glEnableVertexAttribArray(positionLoc)GLES20.glVertexAttribPointer( positionLoc, 2, GLES20.GL_FLOAT, false, 8, 0)다음으로 glBindBuffer(...)를 호출하여 해당 버퍼를 사용하겠다고 지정합니다.glBufferData(...)는 버퍼에 들어갈 데이터를 초기화하는 함수입니다. 뒤의 GLES20.GL_STATIC_DRAW 인자는 이 데이터가 얼마나 자주 수정되고 사용되는지에 대한 값으로, 이 값은 자주 사용되지만 수정하지 않는 데이터에 적용합니다. 이 값에 따라 데이터의 메모리 위치가 결정됩니다.이후 glEnableVertexAttribArray(...)를 호출해 버퍼의 값을 Vertex Shader의 attribute가 읽을 수 있도록 설정하고, glVertexAttribPointer(...)로 값을 어떻게 읽을지 설정합니다. 파라미터는 각각 attribute의 location 값 각 값의 개수 (몇 개씩 들어갈 것인지) 자료형 정규화 여부(0~1 사이의 값으로) 하나의 값이 차지하는 크기(2개씩 각 4바이트(float)이므로 8바이트) 기본값을 의미합니다.in, out 키워드결론부터 적자면, OpenGL ES 2.0에서는 해당 키워드를 GLSL ES(ESSL)에 사용할 수 없습니다.uniform mat4 uMVPMatrix;uniform mat4 uTexMatrix;attribute vec4 aPosition;attribute vec4 aTextureCoord;varying vec2 vTextureCoord;void main() { gl_Position = uMVPMatrix * aPosition; vTextureCoord = (uTexMatrix * aTextureCoord).xy;}위의 vertex shader 코드에서 attribute 대신에, layout(location = 0) in을 사용할 수 있습니다. 이 경우 얻을 수 있는 장점은:private val positionLoc: IntpositionLoc = GLES20.glGetAttribLocation(programHandle, &quot;aPosition&quot;)Vertex Shader의 attribute 변수의 location 값을 가져오기 위해 위와 같은 코드를 작성할 필요가 없어집니다. 대신 location = 으로 지정한 값을 그대로 사용할 수 있습니다. location 값을 가져오는 것이 왜 필요한지 궁금하시다면 바로 위의 Vertex Buffer Object에 대해 작성한 코드에서 positionLoc 값이 어떻게 쓰였는지 확인하시면 도움이 될 것입니다. 만약 in 키워드를 사용한다면 positionLoc 대신에 location으로 지정한 값, 예를 들어 0을 대신 사용할 수 있을 것입니다.또한 out 키워드는 varying 대신에 사용할 수 있습니다. varying은 Vertex Shader의 결과물, 출력값이며 Fragment Shader의 입력값으로 쓰입니다. 이를 Vertex Shader 에서는 out으로 쓰고, Fragment Shader에서 in으로 사용할 수 있습니다.in, out 키워드가 도입된 이유는 더 이상 Shader가 두 종류만 존재하지 않기 때문입니다. OpenGL 3.2에서 Vertex Shader와 Fragment Shader 사이에 Geometry Shader라고 불리는 추가적인 Shader가 추가되었고, 그 이후 버전에서 Tesellation Evaluation Shader, Compute Shader 등이 추가되었습니다. 또한 이러한 Shader들은 Vertex Shader나 Fragment Shader처럼 필수적인 과정이 아니기 때문에 특정 값이 어느 Shader와 연결되는지에 대해서는 알 수 없습니다. 따라서 in, out 키워드로 데이터 입출력을 지정하도록 하여 각 Shader 간의 데이터 연결을 하는 방식으로 변경되었습니다.attribute와 varying 타입 한정자(type qualifier)는 GLSL 1.30에서 deprecated 되었고 GLSL 1.40부터 제거되었습니다. 이는 GLSL ES에서는 3.00부터 이미 제거된 상태에 해당됩니다. 즉 OpenGL ES 3.0 이상을 사용한다면 이 두 타입 한정자를 사용할 수 없습니다.하지만 이 프로젝트에서는 OpenGL ES 2.0도 지원하도록 하고 있고, 버전을 확인해보면 OpenGL ES 2.0은 GLSL ES 1.00 버전을 사용합니다. layout 키워드는 GLSL 3.3 (OpenGL 3.3에서 사용)을 기반으로 한 GLSL ES 3.00에서 추가되었으므로 OpenGL ES 2.0을 사용한다면 해당 키워드를 사용할 수 없습니다.따라서 런타임에 사용 버전을 결정하는 이 앱에서는 해당 키워드를 사용하지 않거나, 버전에 따라서 사용하는 GLSL ES 코드를 분리할 수 있을 것입니다. 현재는 사용에 지장이 없기 때문에 기존의 코드를 그대로 사용하기로 하였습니다만, 분리한 코드로 테스트하였을 때 성능의 향상이 있다면 수정할 예정입니다." }, { "title": "[Android] 동영상에 그림 그리기 (9) - SurfaceView로 화면 표시", "url": "/posts/video_memo_9/", "categories": "Android, boomerang", "tags": "Android, Surface, SurfaceView, TextureView", "date": "2022-01-13 01:15:00 +0900", "snippet": "지난 포스트에서는 MediaCodec에서 생성한 Surface에 비디오 버퍼를 전달하여 동영상을 저장하였습니다. 이번 포스트에서는 동영상을 사용자에게 표시하는 방법에 대해 작성하겠습니다.SurfaceView사용자는 스스로가 그리고 있는 그림을 계속 확인하여야 하기 때문에, 화면에 표시될 것은 원본 동영상이 아닌 그림이 그려진 동영상의 각 이미지 프레임입니다. 지난 포스트에서 그림이 그려진 그래픽 버퍼를 어떻게 SurfaceView의 Surface로 전달하는지에 대해서 작성하였습니다.SurfaceView를 이름과 연관지어 설명하자면 Surface+View로, 뷰 계층구조 내에 추가적인 Surface를 생성하고자 할 때 사용할 수 있습니다.뷰 계층구조 내에 추가적인 Surface가 필요할 때는 언제일까요? 지난 포스트에서 SurfaceFlinger의 각 Layer는 각각의 BufferQueue를 가지고, 이 BufferQueue의 생산자 측에 Surface를 전달하여 그래픽 버퍼를 가져온다고 하였습니다.각 BufferQueue마다 따로 그래픽 버퍼를 전달할 수 있기 때문에, 그래픽 버퍼를 자주 갱신해야 한다면 별개의 Surface를 가져 뷰 계층구조 전체의 그래픽 버퍼를 전달하는 대신 필요한 부분만 전달할 수 있을 것입니다.SurfaceView를 생성하면 SurfaceControl에게 새로운 Surface의 생성을 요청합니다. 그러면 SurfaceFlinger에서 새로운 BufferQueue를 생성하고, Surface를 전달할 것입니다. SurfaceFlinger로부터 전달받은 Surface들을 사용하기 위해서는 SurfaceHolder 인터페이스를 사용하면 됩니다(이 내용은 지난 포스트에서 작성하였습니다.)이렇게 가져온 Surface는 여러 생산자 API에서 사용될 수 있습니다. 대표적인 예로 이 앱에서 사용한 OpenGL ES이나 카메라 미리보기, MediaPlayer 등이 있습니다.TextureViewSurfaceView와 비교할 대상으로 TextureView가 있습니다.SurfaceView와 TextureView 둘 모두가 뷰 계층구조의 일부라는 공통점이 있지만, 실제 구현에 있어서는 큰 차이가 있습니다.TextureView는 SurfaceTexture를 생성할 수 있고, Surface를 생산자에게 전달하여 원하는 그래픽 버퍼를 가져올 수 있습니다. 즉 TextureView의 그래픽을 그리거나 갱신할 때 Surface에 하드웨어 가속을 적용해 그릴 수도 있고, SurfaceTexture를 이용해 OpenGL ES로도 렌더링하는 등 기존 방식보다 성능상의 이점을 가질 수 있습니다.하지만 Surface가 SurfaceView처럼 분리되지 않기 때문에 SurfaceView가 성능에서 더 우위를 가집니다. 뷰의 일부에서 동영상을 재생한다고 가정할 때, SurfaceView를 사용하면 동영상을 재생하는 부분만 별도의 Surface로 분리하여 해당부분의 그래픽 버퍼만 갱신할 수 있습니다. 하지만 TextureView는 Surface가 분리되지 않기 때문에 갱신시에 뷰 계층구조의 다른 컴포넌트들도 함께 갱신되고, 이는 성능 저하의 원인이 될 것입니다.API 24 이전에는 SurfaceView의 분리된 Surface가 예상대로 동기화되지 않는 문제점이 발생하였지만 현재는 수정되었기 때문에 꼭 필요한 경우가 아니라면, SurfaceView의 사용을 권장합니다." }, { "title": "[Android] 동영상에 그림 그리기 (8) - BufferQueue, WindowManager, SurfaceFlinger", "url": "/posts/video_memo_8/", "categories": "Android, boomerang", "tags": "Android, BufferQueue, Surface, SurfaceFlinger, WindowManager", "date": "2022-01-12 02:12:00 +0900", "snippet": "이전 포스트들에서 SurfaceTexture, Surface, MediaCodec 등 미디어, 그래픽에 쓰이는 API들에 대해 다루었습니다.. 이러한 API들의 역할과 사용 방법에 대해서는 다루었지만, 작동 방식에 대해 조금 더 깊이 알아보고 싶어서 Google I/O ‘18의 Drawn out: How Android renders 라는 주제의 발표와 공식 문서의 내용들을 조합하여 정리하려 합니다.이번 포스트에서 다루는 내용들은 개발자가 직접 조작이나 설정해야 하는 부분은 아니지만, 그래픽이 어떤 과정을 거쳐 화면에 표시되는지 이해하는 것에 도움이 될 것입니다.(출처: AOSP 그래픽)BufferQueueBufferQueue는 이름 그대로 (그래픽) 버퍼의 큐입니다. 버퍼의 수는 생성할 때 결정할 수 있습니다.중요한 부분은 BufferQueue는 두 개의 엔드포인트를 가지는데, 생산자와 소비자라고 합니다.(출처: AOSP 그래픽)생산자는 dequeueBuffer()를 호출하여 BufferQueue에서 버퍼를 가져와 소유권을 얻습니다. 그 후 생산자 라는 이름에 맞게 내용을 채워넣을 수 있습니다. 그게 Canvas가 될 수도 있고, OpenGL ES가 될 수도 있으며, 픽셀 데이터를 직접 넣을 수도 있습니다.생산자가 버퍼에 데이터를 모두 넣은 후에는, 다시 BufferQueue에 버퍼를 queueBuffer() 호출로 돌려줍니다. SurfaceTexture 클래스에서 OpenGL ES 렌더링이 끝난 후 eglSwapBuffers()를 호출하는 것도 이 queueBuffer() 과정에 해당합니다.생산자의 역할은 끝났으니, 이제는 소비자 차례입니다. 소비자는 acquireBuffer()를 호출하여 BufferQueue에서 버퍼를 가져옵니다. 소비자는 가져온 버퍼로 원하는 작업을 수행하고(지난 포스트의 MediaCodec을 예로 들자면, 버퍼를 가져와서 MediaMuxer에 작성하였습니다.) releaseBuffer()로 다시 BufferQueue에 버퍼를 돌려줍니다.이렇게 생산자와 소비자가 그래픽 버퍼를 BufferQueue를 통해 교환할 수 있기 때문에, 생산자와 소비자가 다른 성격의 API이더라도 그래픽 데이터를 전달할 수 있습니다. 따라서 이 앱에서 동영상을 재생하는 MediaPlayer로부터 OpenGL ES를 사용하는 SurfaceTexture에 프레임 단위로 동영상을 전달하는 것이 가능했던 것이죠.BufferQueue는 소비자에 해당하는 컴포넌트에서 생성하여, 나머지 하나의 엔드포인트에 해당하는 생산자에게 Surface를 넘겨주어 버퍼를 채우도록 합니다.이전에 다뤘던 Surface는 BufferQueue의 생산자쪽 엔드포인트에 전달하여 그래픽 버퍼를 가져오기 위해 사용된 것입니다. 이 엔드포인트의 반대편, 즉 소비자는 주로 SurfaceFlinger로, Window들의 그래픽 버퍼를 각각 BufferQueue로 모아 화면을 구성하는 역할을 합니다. 또한 제가 이전 포스트들에서 다룬 SurfaceTexture, MediaCodec API도 BufferQueue의 소비자 엔드포인트로 동작할 수 있습니다. SurfaceFlinger를 소비자로 사용하게 되는 방법 중 하나는 SurfaceView API를 사용하는 것인데, 이는 다음 포스트에서 그림이 그려진 동영상을 재생하는 내용을 다루면서 작성하도록 하겠습니다.WindowManagerSurfaceFlinger를 소비자로 하는 생산자는 WindowManager 가 있습니다.SurfaceFlinger는 버퍼를 WindowManager로부터 받아 합성하여 새로운 버퍼를 디스플레이로 보냅니다.WindowManager는 SurfaceFlinger가 Surface를 디스플레이에 합성하는데 사용하는 버퍼와 Window의 메타데이터를 SurfaceFlinger에 제공합니다.우리가 Dialog, Toast, Activity와 같은 요소들을 만들 때, Window 객체가 생성됩니다. 그리고 SurfaceFlinger 쪽에도 Window에 해당하는 객체가 있는데, 이를 Layer 라고 합니다.Layer는 소비자의 역할로 BufferQueue를 생성하고 소유합니다. 그리고 Surface를 생산자에 해당하는 API들에게 전달하여 그래픽 버퍼들을 SurfaceFlinger로 가져올 수 있게 됩니다.SurfaceFlinger(출처: AOSP 그래픽)SurfaceFlinger는 이와 같이 Layer로부터 그래픽 버퍼를 가져옵니다. 이러한 Layer는 하나가 아닌 여러 개이고, 여러 소스로부터 그래픽 버퍼를 가져옵니다.위 그림처럼 홈 화면이나 상태 표시줄, 시스템 UI 등은 각각의 BufferQueue를 통해 따로 그래픽 버퍼가 들어오게 됩니다.생산자들은 그래픽 버퍼에 데이터를 계속 입력할 수 있지만, 소비자인 SurfaceFlinger는 BufferQueue에서 그래픽 버퍼를 항상 가져오지 않습니다. 디스플레이 새로고침 사이에서만 버퍼를 가져옵니다. 이 새로고침 주기는 일반적으로 초당 60번, 즉 60fps로 동작하고 1프레임은 약 16ms동안 지속됩니다.이렇게 새로운 버퍼를 탐색할 수 있도록 새로고침 신호를 주는 것을 VSYNC라고 합니다.SurfaceFlinger는 (60fps로 가정하고)16ms라는 짧은 시간 내에 Layer 목록에서 새로운 버퍼를 찾아 갱신하고, 새 버퍼가 없다면 기존의 버퍼를 계속 사용합니다.그래픽 버퍼를 모두 수집한 후에는 하드웨어 컴포저(HWC)에 합성 유형을 묻고, SurfaceFlinger가 이에 해당하는 Layer를 합성한 후, 출력 버퍼를 하드웨어 컴포저에 전달합니다.이 이후 구성은 하드웨어에 가까운 내용이기도 하고, 저의 현재 지식 수준으로는 이해하기 어려운 내용이 많아 여기까지 작성하겠습니다. 이후 내용은 하드웨어 추상 계층(HAL)을 통해 디스플레이 하드웨어 OEM에 의해 수행됩니다.(참고자료: SurfaceFlinger 및 WindowManager)" }, { "title": "[Android] 동영상에 그림 그리기 (7) - MediaCodec으로 인코딩", "url": "/posts/video_memo_7/", "categories": "Android, boomerang", "tags": "Android, MediaCodec, MediaMuxer", "date": "2022-01-05 00:00:00 +0900", "snippet": "이번 포스트에서는 MediaCodec과 MediaMuxer를 사용하여 렌더링된 그래픽들을 동영상으로 합성하는 내용에 대해 작성하도록 하겠습니다.MediaCodecMediaCodec은 인코딩, 디코딩 기능을 하는 클래스입니다.MediaCodec 클래스에서 사용하는 데이터는 ByteBuffer로도 가능하지만, 비디오 데이터는 지난 포스트들에서 그래픽 버퍼 전달에 쓰여왔던 Surface를 사용하는 것이 코덱 성능에 이롭습니다. 지난 포스트들에서 겪었던 과정들을 돌아보면 Surface 클래스를 사용하면서 그래픽 버퍼를 이동시키기 위해 복사를 하거나 매핑하는 과정을 거칠 필요가 없었기 때문에 효율적이라고 할 수 있습니다.또한 Surface를 사용하는 것이 구현에 있어서도 더 편합니다. MediaCodec의 입력으로 Surface를 사용하면, 자동으로 버퍼를 코덱에 연결하기 때문에 입력 버퍼와 관련된 함수를 사용할 필요가 없을 뿐만 아니라, 색 영역에 대한 포맷 설정이 매우 간단해집니다.H.264(AVC) 포맷으로 인코딩시에는 YUV420 값이 필요하고, 이 포맷은 기기의 색상값에 따라 다르기 때문에 확인을 해야 하는 등 복잡한 과정이 필요합니다. 하지만 Surface를 사용한다면, 아래와 같이 색 영역 포맷 설정을 끝낼 수 있습니다.format.setInteger( MediaFormat.KEY_COLOR_FORMAT, MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface)Surface로 전달받는 비디오 버퍼의 색 영역 포맷을 그대로 사용할 수 있습니다. 그렇다면 색 영역 포맷은 어디서 설정했을까요? 이전 포스트에서 EGLConfig를 설정할 때 RGBA_8888 로 설정했습니다. 이 값이 그대로 적용될 것입니다.또한 Surface 사용시 동영상이 끝나는 지점을 signalEndOfInputStream() 함수로 알 수 있기 때문에 종료 시점을 아는 것에 있어 유용합니다.정리하자면, Surface 클래스를 사용하는 것이 ByteBuffer를 사용하는 것 보다 인코딩/디코딩에 있어 편리합니다.MediaCodec 클래스는 createInputSurface() 함수로 Surface 인스턴스를 생성하여 인코더의 입력으로 사용할 수 있습니다. 이는 지난 포스트에서 SurfaceTexture가 생성한 Surface 인스턴스를 MediaPlayer에게 주어 동영상의 이미지 버퍼를 받던 것과 같은 방식이라고 생각할 수 있습니다.하지만 createInputSurface() 전에 인코더의 포맷을 설정해야 합니다.val encoder = MediaCodec.createEncoderByType(&quot;video/avc&quot;)우선 인코더를 희망하는 MIME 타입에 맞게 생성하고, 포맷 설정을 살펴보도록 하겠습니다.MediaFormat필수적으로 설정하여야 하는 포맷은 MediaFormat 레퍼런스 페이지를 참조하면 됩니다.val format = MediaFormat.createVideoFormat(&quot;video/avc&quot;, width, height)우선 최소한의 설정을 담은 비디오 포맷을 생성합니다. MIME, 너비, 높이 값(픽셀 단위)이 파라미터로 필요합니다. 이후부터는 format.setInteger(...)로 포맷을 더해가면 됩니다.format.setInteger( MediaFormat.KEY_COLOR_FORMAT, MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface)format.setInteger(MediaFormat.KEY_BIT_RATE, bitrate)format.setInteger(MediaFormat.KEY_FRAME_RATE, frameRate)format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, 1)KEY_COLOR_FORMAT은 위에서 작성한대로 Surface의 포맷을 따르도록 하였습니다.KEY_BIT_RATE는 비트레이트 값을 설정합니다. 안드로이드에서 H.264(AVC) 인코딩 시에 권장하는 스펙은 공식 문서 에서 확인할 수 있습니다.KEY_FRAME_RATE는 목표로 하는 초당 프레임 수를 설정합니다. 하지만 실제 값은 timestamp에 의해 각 프레임이 자리하는 시간이 바뀌기 때문에, 여기서 설정한 FPS 값대로 동영상이 출력된다는 보장은 없습니다.KEY_I_FRAME_INTERVAL은 Intra Frame이 몇 초에 하나씩 위치할 것인지를 설정합니다. 인코딩 시에는 모든 프레임이 완전한 그림의 형태를 가지지 않고, 이러한 Intra Frame와의 변화값만을 가져 크기를 압축합니다. 하지만 Intra Frame이 너무 적다면, 영상을 재생하다가 다른 부분을 보고 싶어서 중간으로 이동할 때, 해당 프레임을 알기 위해서 Intra Frame을 찾을 때 목표 위치와 크게 떨어질 수 있고, 그 결과 재생에 오랜 시간이 걸리게 될 것입니다. 따라서 적절한 값을 설정하여야 합니다.KEY_I_FRAME_INTERVAL 값을 음수로 설정하면 첫 번째 프레임을 제외한 어떠한 Intra Frame도 설정하지 않으며, 값을 0으로 설정시에는 모든 프레임이 Intra Frame이 됩니다.포맷 설정을 마치고 나면, 이제 createInputSurface()로 Surface 인스턴스를 생성할 수 있을 것입니다.MediaCodec 설정encoder.configure(format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE)inputSurface = encoder.createInputSurface()encoder.start()createInputSurface()는 레퍼런스 문서에서 확인할 수 있듯이, configure(...) 이후, start() 이전에 위치하여야 합니다.configure(…)는 MediaFormat, Surface, MediaCrypto, flags: Int 를 필요로 합니다. MediaFormat은 위에서 설정한 포맷이 그대로 들어가면 되고, Surface와 MediaCrypto는 인코더에서는 사용하지 않으므로 null, 인코더로 사용을 알리는 flag인 CONFIGURE_FLAG_ENCODE을 각각 인자로 넣었습니다.모든 설정이 끝난 후, start() 함수를 호출하면 됩니다.인코딩val bufferInfo = MediaCodec.BufferInfo()while (true) { when (val encoderStatus = encoder.dequeueOutputBuffer(bufferInfo, 0)) { MediaCodec.INFO_TRY_AGAIN_LATER -&amp;gt; break MediaCodec.INFO_OUTPUT_FORMAT_CHANGED -&amp;gt; { encodedFormat = encoder.outputFormat videoTrack = mediaMuxer.addTrack(encodedFormat) mediaMuxer.start() isMuxerStart = true } else -&amp;gt; { val encoderOutputBuffers = encoder.getOutputBuffer(encoderStatus) ?: throw Exception(&quot;MediaCodec.getOutputBuffer is null&quot;) if (bufferInfo.flags and MediaCodec.BUFFER_FLAG_CODEC_CONFIG != 0) bufferInfo.size = 0 if (bufferInfo.size != 0) { if (isMuxerStart) mediaMuxer.writeSampleData( videoTrack, encoderOutputBuffers, bufferInfo ) } encoder.releaseOutputBuffer(encoderStatus, false) if (bufferInfo.flags and MediaCodec.BUFFER_FLAG_END_OF_STREAM != 0) break } }}dequeueOutputBuffer는 출력 버퍼의 인덱스를 반환합니다. 두 개의 파라미터가 필요한데, 첫 번째는 MediaCodec.BufferInfo로, 버퍼의 메타데이터가 채워지게 됩니다. 두 번째로는 타임아웃입니다. 타임아웃 값을 0으로 설정하였는데 이 경우 버퍼를 확인하고 없으면 기다리지 않고 바로 INFO_TRY_AGAIN_LATER 값을 반환합니다.이 과정이 성공적으로 끝났다면 이후로는 getOutputBuffer로 출력 버퍼의 데이터를 ByteBuffer로 가져옵니다. 이제 dequeueOutputBuffer(...)에서 가져온 BufferInfo의 메타데이터를 확인하여 유효한 값인지 확인하여야 합니다.우선 bufferInfo.flags 와 MediaCodec.BUFFER_FLAG_CODEC_CONFIG == 2의 비트연산입니다. 비트연산에서 0이 나오지 않으면 size를 0으로 만들어 다음에서 MediaMuxer에 버퍼를 넘기지 않도록 구현이 되어있습니다. 다른 일반적인 flags는 2와의 비트연산 결과에서 0이 나올 수 있도록 값을 가지고 있습니다. MediaCodec.INFO_TRY_AGAIN_LATER == -1과 MediaCodec.INFO_OUTPUT_FORMAT_CHANGED == -2 둘은 연산결과에서 0이 아닌 값을 가지게 되는데, dequeueOutputBuffer의 반환값을 구분했던 것과 같습니다.다음으로 MediaMuxer에 출력 버퍼의 데이터를 전달하고, 출력 버퍼를 release 합니다. 이 작업을 매 프레임마다 반복합니다.MediaMuxerMediaCodec의 결과물이 바로 .MP4 등의 파일로 출력되는 것은 아닙니다. 포맷에 맞게 인코딩은 완료되었지만, 이제 파일로 만드는 부분을 MediaMuxer가 담당하게 됩니다.인코딩 데이터는 writeSampleData 함수에 ByteBuffer 데이터와 MediaCodec.BufferInfo 메타데이터를 파라미터로 전달하게 됩니다.종료인코딩을 마치고 나면, MediaCodec과 MediaMuxer 각각을 종료해야 합니다.MediaCodec에서의 stop은 인코딩/디코딩 후 호출하면 됩니다. stop() 호출 뒤에도 다시 start() 함수를 호출하여 다시 시작할 수 있습니다.MediaCodec에서 사용하였던 모든 리소스들을 정리하려면 release를 호출하여야 합니다. 인코딩/디코딩 작업이 끝났을 때 메모리 정리를 가비지 컬렉터에 의존하지 말고, 직접 release() 함수를 호출하여야 한다고 공식 문서에 명시되어 있습니다.MediaMuxer에서의 stop은 약간 다릅니다. 호출 후에는 다시 시작될 수 없습니다.MediaMuxer에서의 release는 MediaCodec의 release처럼 사용한 리소스들을 정리합니다.두 클래스의 인스턴스에 대하여 각각 stop(), release() 순서로 호출하면 됩니다." }, { "title": "[Android] 동영상에 그림 그리기 (6) - Framebuffer 전달 및 복사", "url": "/posts/video_memo_6/", "categories": "Android, boomerang", "tags": "Android, OpenGL ES, EGL", "date": "2022-01-02 00:00:00 +0900", "snippet": "OpenGL ES의 렌더링 과정을 모두 마치면, EGLSurface에 Framebuffer가 들어가게 됩니다. 그런데 이 앱에서 Framebuffer가 필요한 Surface는 두 곳이 있습니다. 하나는 화면에 표시하기 위한 SurfaceView이고, 다른 하나는 동영상 저장을 위한 MediaCodec 입니다. 이 과정에서 제가 구현한 방법과 각 방법에 대해 실험한 결과에 대해 작성하겠습니다.Surface 전달 방법렌더링 두 번 하기가장 먼저 떠오르는 방법은 각 Surface마다 렌더링을 해서 Framebuffer를 전달하는 것입니다.eglCreateWindowSurface(…)는 Surface를 포함한 파라미터를 받습니다. SurfaceView와 MediaCodec이 전달한 Surface를 통해 EGLWindowSurface를 생성합니다.eglMakeCurrent(…) 함수를 사용하여 렌더링 대상이 될 EGLSurface를 선택할 수 있습니다.이 방법은 SurfaceView의 Surface에 렌더링을 한 번 하고, 렌더링될 Surface를 바꾼 후 다시 렌더링을 합니다. 하나의 같은 프레임에 대해 렌더링을 두 번을 수행하기 때문에, 낭비라는 생각이 들 수 있습니다.Framebuffer 복사OpenGL ES 3.0 이상에서는 glBlitFramebuffer(…) 함수를 사용하여 Framebuffer간 특정 영역의 픽셀을 복사할 수 있습니다. 전체 영역을 복사한다면 렌더링을 다시 할 필요가 없을 것입니다.fullFrameBlit.drawFrame(textureId, transformMatrix)drawLine(currentPoint, height)if (egl.glVersion == 3) {// SurfaceView에 그릴 Framebuffer를 아직 swap하지 말고 인코딩 버퍼에 복사하고 둘 다 swapencoderSurface.makeCurrentReadFrom(displaySurface)GLES30.glBlitFramebuffer( 0, 0, displaySurface.width, displaySurface.height, 0, 0, displaySurface.width, displaySurface.height, GLES30.GL_COLOR_BUFFER_BIT, GLES30.GL_NEAREST)videoDoodleViewModel.encoder.transferBuffer()encoderSurface.setPresentationTime(surfaceTexture.timestamp)encoderSurface.swapBuffers()displaySurface.makeCurrent()displaySurface.swapBuffers()} else {// OpenGL ES 2.0일 경우. glBlitFramebuffer를 지원하지 않기에 그냥 두 번 그린다.displaySurface.swapBuffers()encoderSurface.makeCurrent()GLES20.glViewport(0, 0, width, height)fullFrameBlit.drawFrame(textureId, transformMatrix)drawLine(currentPoint, height)videoDoodleViewModel.encoder.transferBuffer()encoderSurface.setPresentationTime(surfaceTexture.timestamp)encoderSurface.swapBuffers()displaySurface.makeCurrent()}이 과정을 위 코드로 구현하였습니다. glBlitFramebuffer(...)을 사용할 수 없는 OpenGL ES 2.0 에서는 렌더링을 두 번 하는 방식을 그대로 쓰고, glBlitFramebuffer(...)를 사용할 수 있는지 알기 위해 EGL 초기화 과정에서 가져온 OpenGL ES 버전에 따라 Framebuffer 전달 방식을 나누어 구현하였습니다.이 방식에서는 렌더링을 한 번만 수행하고, 대신 glBlitFramebuffer(...) 과정이 추가되었습니다. 그렇다면 두 방법 중 어떤 방법이 더 빠른 속도로 작업을 마칠 수 있을까요?각 방법의 초당 프레임 측정이 측정을 하게 된 이유는 기존에는 위의 렌더링을 두 번 하는 방식만 사용하였는데, 프레임 저하 현상을 크게 느낄 수 있었기 때문입니다. 그래서 개선될 것으로 예상되는 방법을 찾고 비교해보았습니다.두 번 렌더링하는 방식30fps의 원본 동영상을 두 번 렌더링하는 기존 방식을 거쳐 다시 동영상으로 저장하였을 때, 사용자가 입력을 하지 않고 그대로 렌더링을 한다면 29.5fps로 원본과 큰 차이가 없었습니다. 하지만 사용자가 메모를 작성하였을 때에는 20.1fps로 큰 차이가 발생하였습니다. 한 번 렌더링 하는데 걸리는 시간이 길어지다보니 두 번 렌더링 하는 방식에서는 초당 프레임 감소가 더 심해졌던 것으로 추정됩니다.Framebuffer 전달 방식 개선 결과glBlitFramebuffer(...)를 사용한 방법으로 변경 후 측정 결과입니다.메모를 그리지 않고 동영상만 그대로 다시 렌더링하였을 때에는 오히려 개선 전보다 개선 후의 초당 프레임이 2 가량 낮아졌습니다.하지만 동영상에 그림을 그리면서 렌더링을 한 결과는 변경 후 초당 프레임이 6 이상 높아졌습니다.분석측정 결과 모든 상황에서 복사가 렌더링보다 빠른 것은 아니었습니다.위 그림은 측정 결과에 대해 저의 추정을 표현한 것입니다. 렌더링에 걸리는 시간은 그림을 그릴수록 더 길어질 것입니다. 그에 비해 glBlitFramebuffer(...)의 실행 시간은 그림의 여부와는 상관없이 전체 픽셀을 복사하기 때문에 일정할 것입니다.따라서 동영상을 그대로 다시 렌더링하였을 때에는 렌더링의 소요 시간이 glBlitFramebuffer(...)보다 짧았던 것이고, 메모를 그렸을 때에는 렌더링의 소요 시간이 길어져 glBlitFramebuffer(...) 보다 길어졌기 때문에 위와 같은 결과가 나왔다고 생각됩니다.다음 포스트에서는 MediaCodec에서 Surface를 사용하여 전달 받은 이미지 버퍼를 동영상으로 인코딩하는 과정에 대해 작성하겠습니다." }, { "title": "[Android] 동영상에 그림 그리기 (5) - OpenGL ES의 Shader, 그림 그리기", "url": "/posts/video_memo_5/", "categories": "Android, boomerang", "tags": "Android, OpenGL ES, Shader", "date": "2022-01-01 00:00:00 +0900", "snippet": "렌더링을 위해서는 Shader가 필요합니다. 이번 포스트에서는 Shader에 대해 작성하도록 하겠습니다.Shader렌더링 파이프라인에서 Vertex Shader와 Fragment Shader는 필수적으로 사용자가 직접 구현해야 하는 부분입니다. GLSL이라는 언어를 사용하여 구현하여야 합니다.Vertex Shader입력되는 점마다 하나씩 수행됩니다. 예를 들어 삼각형을 하나 그린다고 하면 3개의 점이 필요하고, Vertex Shader는 총 세 번 수행되는 것입니다.주된 역할은 점의 위치를 결정하는 것입니다. 각 점들은 NDC라고 하는 각 축마다 [-1.0, 1.0] 사이로 정의된 공간 내에 위치하도록 합니다. 2차원 좌표계에서는 왼쪽 아래가 (-1, -1)이고 오른쪽 위가 (1, 1)의 값을 가집니다.도형에 별도로 색상을 넣지 않고 텍스처만 입힐 것이기 때문에 색상은 따로 지정하지 않았습니다.uniform mat4 uMVPMatrix;uniform mat4 uTexMatrix;attribute vec4 aPosition;attribute vec4 aTextureCoord;varying vec2 vTextureCoord;void main() { gl_Position = uMVPMatrix * aPosition; vTextureCoord = (uTexMatrix * aTextureCoord).xy;}4X4 Matrix에 4X1 Vertex를 곱해 새로운 Vertex 값을 출력할 수 있습니다.MVPMatrix는 OpenGL ES에서 렌더링한 객체들을 사실적으로 표현하기 위해 조정하는 변환행렬입니다.TexMatrix는 SurfaceTexture에서 getTransformMatrix(...)로 받은 변환행렬 정보입니다. 이미지가 전달되는 과정에서 뒤집히는 등의 변형이 발생해도, 변환행렬에 저장되어 있기 때문에 여기서 수정할 수 있습니다.varying은 Vertex Shader의 출력이자 Fragment Shader의 입력으로 쓰일 수 있는, 두 Shader가 공유하는 형태의 변수입니다. 여기서 Fragment Shader로 보내야 할 정보는 텍스처 좌표(Texture Coordinate)입니다. 텍스처는 2차원 벡터이기 때문에(z축 값과 w 값을 제외한) vec2 형입니다.gl_Position은 Vertex Shader의 출력으로, 결정된 Vertex의 위치를 내보내 이후 Primitive Assembly 단계에서 점, 선, 삼각형을 구성합니다.Fragment Shader#extension GL_OES_EGL_image_external : requireprecision mediump float;varying vec2 vTextureCoord;uniform samplerExternalOES sTexture;void main() { gl_FragColor = texture2D(sTexture, vTextureCoord);}지난 포스트에서 외부 GLES 텍스처를 사용하였기 때문에, 첫 번째 줄의 #extension을 선언하여야 합니다.precision은 실수부 연산의 정확도를 지정합니다. mediump는 16bit를 사용합니다(lowp: 10bit, highp: 32bit).texture2D 함수는 텍스처에서 좌표(Vertex Shader으로부터 입력으로 받은 vTextureCoord)에 해당하는 텍셀의 색상 값을 반환합니다.Vertex BufferVertex Shader에 정점에 대한 입력값을 주기 위해서는 Vertex Buffer를 사용해야 합니다.위 코드는 제가 GLSL에 대한 지식도 없고, OpenGL ES에 대한 지식도 부족했을 때 구글의 그래픽, 미디어 관련 예제 모음인 Grafika의 코드를 그대로 사용하였습니다. 굉장히 훌륭한 예제이고, Google I/O에서도 코드를 살펴볼 것을 권하고 있습니다. 하지만 프로젝트의 README 에도 적혀있듯이 구글의 공식 프로젝트도 아니고, 모든 부분이 최적화된 방법으로 작성된 코드도 아닙니다.이 부분은 제 의견이 틀릴 수도 있지만(다른 생각을 가지고 계시다면 메일 등을 통해 알려주시면 대단히 감사하겠습니다), StackOverflow 등에서의 여러 의견을 종합한 내용들을 읽어보니 Buffer에 관해서는 개선될 여지가 있을 것으로 보입니다.버퍼를 사용하는 방법은 두 가지가 있습니다. java.nio 패키지의 ByteBuffer를 사용할 수도 있고, OpenGL ES에서 버퍼를 생성할 수도 있습니다. 두 방식의 주요한 차이점은 메모리의 저장 위치에 있습니다. Java와 같이 클라이언트에서 생성한 버퍼는 CPU에서 접근 가능한 시스템 메모리에 위치하고, 이를 렌더링에 사용하기 위해서는 비디오 카드가 접근 가능한 메모리로 옮겨야 합니다.OpenGL ES에서 제공하는 glBufferData를 통해 버퍼를 할당하면 담을 Vertex Array의 수정과 접근 빈도에 따라 GL_STATIC_DRAW, GL_DYNAMIC_DRAW, GL_STREAM_DRAW 등의 옵션이 주어집니다. 이 앱에서 점의 위치는 화면 전체로 고정하고, 텍스처만 계속 바뀌기 때문에 Vertex Array는 바뀌지 않습니다. 이 경우 GL_STATIC_DRAW를 적용할 수 있을 것입니다. Grafika에서는 Java의 패키지를 사용하는 전자의 방식을 사용하였는데, 이렇게 클라이언트의 버퍼를 사용하는 것은 OpenGL ES 3.2 에서는 사용이 불가능합니다.추가: OpenGL ES에서 버퍼를 생성하는 방법에 대해서는 프로젝트 개선을 참고해주시기 바랍니다.그림 그리기이 앱은 이미지를 그대로 렌더링해서 보여주는 것이 아니라 그림을 그려서 보여주고, 저장해야 합니다. 제가 그림을 그리기 위해 생각한 방식은 아래 그림과 같이 두 가지 방식이 있습니다.왼쪽의 방법은 Vertex Array에 Vertex 값을 넣고, Vertex Shader에서 각 점에 대해 너비를 계산해야 합니다. 제가 이 부분을 구현할 당시에는 OpenGL ES와 Shader에 대한 이해가 지금보다도 부족할 때여서 구현에 어려움이 있었고, 그래서 오른쪽 방식을 사용했습니다.// 사용자로부터 입력받을 때val last = currentPoint.last()for (i in 1..50) { currentPoint.add( Triple( last.first + (x - last.first) * i / 100, last.second + (y - last.second) * i / 100, drawColor ) )}// 그릴 때currentPoint.forEach { GLES20.glClearColor(it.third.red, it.third.green, it.third.blue, 1f) GLES20.glEnable(GLES20.GL_SCISSOR_TEST) GLES20.glScissor(it.first, height - it.second, 15, 15) GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT) GLES20.glDisable(GLES20.GL_SCISSOR_TEST)}사용자로부터 입력받은 좌표 사이에 여러 개의 좌표를 더해 부드럽게 이어지는 것처럼 보이게 합니다. 그리고 그림을 그릴 때에는 Scissor Test에서 Fragment를 잘라내는 것을 이용해 각 좌표마다 glScissor로 잘라냈습니다.Shader는 저에게 있어 아직 공부가 부족한 부분입니다. 추가로 학습하게 되면 내용을 보충, 개선하겠습니다. 다음 포스트에서는 이렇게 렌더링된 이미지를 어떻게 다른 인스턴스에 전달하는지에 대해 작성하겠습니다." }, { "title": "[Android] 동영상에 그림 그리기 (4) - OpenGL ES의 Texture", "url": "/posts/video_memo_4/", "categories": "Android, boomerang", "tags": "Android, OpenGL ES, SurfaceTexture, Texture", "date": "2021-12-31 00:00:00 +0900", "snippet": "이제 그림을 그릴 시간입니다. Android에서 OpenGL ES 렌더링 과정, 특히 SurfaceTexture를 사용하여 이미지를 텍스처로 사용하는 과정에 대해 정리하겠습니다.SurfaceTextureTexture 초기화지난 글에서 onFrameAvailableListener를 통해 새로운 이미지 프레임을 받은 것을 알 수 있다고 하였습니다. 우선 SurfaceTexture의 생성자부터 하나씩 확인해보도록 하겠습니다.SurfaceTexture(texName: Int)SurfaceTexture를 생성하기 위해서는 외부에서 OpenGL 텍스처를 생성하여 주입시켜야 합니다. texName에서 glGenTextures로 생성된 텍스처의 이름이 들어가야 합니다. GLES20.glGenTextures는 아래와 같이 만들 수 있습니다.GLES20.glGenTextures(n: Int, textures: IntArray, offset: Int) n: 생성할 텍스처의 수 textures: 생성된 텍스처가 저장될 배열. 빈 배열로 생성해두되, n 이상의 크기를 가지고 있어야 할 것입니다. offset: 기본값제가 만든 앱을 예로 들자면, 이미지는 배경으로 깔아줄 것이기 때문이 텍스처는 하나만 있으면 되니val textures = IntArray(1)GLES20.glGenTextures(1, textures, 0)이렇게 쉽게 생성할 수 있습니다. 하지만 생성만 한다고 해서 사용할 수 있지는 않습니다. 다음으로 어떤 대상을 텍스처를 쓸 것인지에 대해 바인딩하는 작업이 필요합니다. glBindTexture를 사용하여 위에서 생성한 텍스처에 어떤 종류의 텍스처가 될 것인지를 지정해야 합니다. 여기서 주의할 점은 Android의 SurfaceTexture 클래스에서 쓰는 텍스처는 일반적인 텍스처의 종류와는 별개라는 것입니다.위의 glBindTexture 링크에서는 텍스처의 종류인 target 으로 GL_TEXTURE_2D, GL_TEXTURE_3D, GL_TEXTURE_2D_ARRAY, GL_TEXTURE_CUBE_MAP 중 하나를 선택하라고 하지만 AOSP SurfaceTexture와 SurfaceTexture 페이지에서는 외부 GLES 텍스처(GL_TEXTURE_EXTERNAL_OES)를 사용하라고 명시되어 있습니다.GL_OES_EGL_image_external은 GL_TEXTURE_2D와 몇 가지 차이점이 있습니다. 우선 대부분의 텍스처 관련 함수들(예를 들어 glTexImage() 등)을 사용할 수 없습니다. 그렇다면 왜 사용하는 것일까요? 바로 렌더링 속도에서 이득을 얻을 수 있기 때문입니다. 다른 텍스처들과는 다르게, 외부 GLES 텍스처는 EGLImage를 텍스처로 바꾸는 방법을 사용합니다. 이 말은, 이미지를 텍스처에 할당하는 과정이 생략될 수 있다는 것입니다. 다른 텍스처 타겟을 사용한다면 glTexImage2D(...)등의 함수를 이용하여 이미지를 텍스처에 할당하는 과정이 필요하게 됩니다.val texture = textures[0]GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, texture)glGenTextures(...)로 가져온 텍스처를 glBindTexture(...)의 texture로 사용하면 됩니다.다음으로는 텍스처에 몇 가지 속성을 지정할 수 있습니다. 저는 텍스처 필터링(Texture Filtering)과 텍스처 래핑(Texture Wrapping) 두 가지 속성을 지정하였습니다.const val textureTarget = GLES11Ext.GL_TEXTURE_EXTERNAL_OESGLES20.glTexParameteri(textureTarget, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_LINEAR)GLES20.glTexParameteri(textureTarget, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_NEAREST)glTexParameteri(…) 함수를 통해 텍스처와 파라미터, 파라미터 값을 지정할 수 있습니다. 뒤에 붙은 i는 Int를 나타내며, f로 바꿔 Float 값을 사용할 수도 있습니다.우선 위의 두 줄, GL_TEXTURE_MAG_FILTER와 GL_TEXTURE_MIN_FILTER는 각각 확대와 축소를 위한 필터입니다.GL_LINEAR는 주변의 2*2 텍셀 값의 평균으로 하는데, 가까운 텍셀의 값에 가중치를 두는 방식을 사용합니다. 확대를 할 때 GL_NEAREST를 사용한다면 가장 가까운 값만을 가져오게 되므로 경계가 뚜렷해져 흔히 말하는 깨져보이는 현상이 발생하게 됩니다. 따라서 확대 필터는 GL_LINEAR 방식을 사용하였습니다.GL_NEAREST는 OpenGL의 필터링 기본값으로, 목표 위치에 가장 가까운 텍셀(텍스처에서의 화소. 렌더링이 완료된 후 프레임버퍼에서의 화소는 픽셀이라고 함)의 값을 가지게 됩니다. 축소 필터 사용시에는 더 작아지기 때문에 깨져보이는 것에 대한 걱정이 없습니다. 따라서 원래의 색상을 그대로 사용하는 이 방식을 적용하였습니다.다음 두 줄은 텍스처 래핑 파라미터를 지정합니다. 텍스처를 입힐 도형과 이미지의 크기가 맞지 않을 때 어떻게 처리할 것인지에 대한 속성입니다. 2차원 평면에서의 사각형에 텍스처를 입힐 때에는 쓰일 일이 크게 없을 것으로 생각됩니다. 주로 원의 끝부분, 또는 양 끝이 이어지는 형태에서 끝 부분 처리에 쓰일 것으로 생각됩니다.이제 OpenGL ES 텍스처가 생성되었습니다. SurfaceTexture의 생성자에 이 텍스처를 넣어 인스턴스가 생성되면, 이제 이미지를 가져올 차례입니다.updateTexImagesurfaceTexture.updateTexImage()이미지를 가져오는 것은 updateTexImage() 함수를 사용하면 됩니다. 그러면 SurfaceTexture에 Surface로 연결된 이미지 스트림 생산자에서 전달한 가장 최신의 이미지를 가져오게 되고, SurfaceTexture 생성시에 사용한 텍스처에 담기게 됩니다.getTransformMatrixprivate val transformMatrix = FloatArray(16)surfaceTexture.getTransformMatrix(transformMatrix)updateTexImage()으로 새로운 이미지를 가져올 때, 변환 행렬 또한 새롭게 가져옵니다. SurfaceTexture에 이미지 버퍼를 보낼 때, 방향이 잘못된 경우가 발생할 수 있습니다. 이 때 방향을 수정해서 SurfaceTexture에 보내기 보다는, 그대로 보내면서 고쳐야 할 정보에 대해서만 알려주고 렌더링 과정에서 고치도록 하는 것이 더 효율적이기 때문에 이런 방식을 사용한다고 합니다.getTransformMatrix(...) 함수를 사용해서 이미지의 변환 행렬을 4*4 행렬에 저장합니다. 각 좌표를 (x, y, z, w) 의 동차좌표 형식으로 표현하기 때문에 변환 행렬을 사용하여 쉽게 변환할 수 있습니다. x, y, z는 3차원을 표현하고 w 값은 0일 때 방향, 1일 때 위치를 의미합니다. 이미지를 표현할 때에는 2차원이기 때문에 (x, y, 0, 1)으로 사용할 수 있습니다.이미지를 텍스처로 가져오는 과정은 여기까지입니다. 다음 포스트에서는 렌더링 과정에 대해 작성하도록 하겠습니다." }, { "title": "[Android] 동영상에 그림 그리기 (3) - OpenGL ES, EGL", "url": "/posts/video_memo_3/", "categories": "Android, boomerang", "tags": "Android, OpenGL ES, EGL", "date": "2021-12-23 00:00:00 +0900", "snippet": "OpenGL ES그림을 그리기 위해서는, 어떻게 그림이 그려지는지 알 필요가 있습니다. 저도 이 과정에 대해 감히 모든 것을 알고 정리한다고 말씀드릴 수는 없을 것입니다. 제가 이 앱을 만들기 위해서 공부했던 범위에서 정리하도록 하겠습니다.렌더링 파이프라인OpenGL ES 3.0의 전체 렌더링 파이프라인은 크로노스 그룹 웹페이지에서 링크를 통해 찾아볼 수 있습니다. 저는 OpenGL ES를 사용하면서 아래 그림과 같이 최소한으로 간단하게 정리하여 생각하였습니다.이 중에서 사각형 상자 내의 Vertex Processing 과 Fragment Processing 과정에서 Shading 이라는 부분을 직접 프로그래밍이 가능합니다. Vertex Shader에서는 점의 위치를 결정하고, Fragment Shader에서는 색상을 결정합니다. 다만 Kotlin이 아닌 GLSL 이라는 별도의 언어를 사용해야 합니다.물론 프로그래밍이 가능하지 않다는 것이 어떠한 조작도 불가능하다는 의미는 아닙니다. 제공되는 함수를 통해 기능을 조작 가능하지만, GLSL 등의 별도의 프로그래밍 언어를 사용하지 않는다는 것이고, 조작의 범위또한 제한적입니다.버전OpenGL ES는 여러 버전이 존재합니다. Android에서 특정 버전을 지원하는지 알기 위해서는 두 가지를 확인해야 합니다.첫 번째로 API 레벨입니다. OpenGL ES 2.0은 API 8 이상에서 지원되고, OpenGL ES 3.0은 API 18 이상에서 지원됩니다. 이렇게만 보면 사실상 거의 모든 기기에서 OpenGL ES 3.0을 지원할 것으로 예상되고 실제로도 그렇기는 하지만 한 가지 더 확인해야 할 사항이 있습니다.바로 GPU의 지원 여부입니다. GPU마다 지원하는 OpenGL ES의 버전이 다르기 때문에 기기에 따라 지원 여부가 달라질 수 있습니다. 물론 OpenGL ES 3.0이 2012년 8월에 공개된 것을 감안하면 거의 모든 기기가 지원하기는 하지만 Android 개발자 문서의 통계에 따르면 2020년 8월에도 OpenGL ES 2.0은 12%의 점유율을 가지고 있습니다.다행히 하위호환이 되어 OpenGL ES 3.X를 지원하는 기기에서 OpenGL ES 2.0을 사용할 수 있습니다(OpenGL ES 1.X까지 지원하지는 않습니다. OpenGL 1.X와 2.0은 렌더링 파이프라인에 큰 차이가 있습니다. 1.X까지는 고정 파이프라인을 사용하다가 2.0부터 이를 제거하고 Shader 파이프라인, 즉 프래그래밍이 가능한 파이프라인으로 변경되었습니다).그렇다면 OpenGL ES 2.0을 지원하는 기기에서 실행이 가능하도록 하고, OpenGL ES 3.X를 지원한다면 추가적인 기능을 사용하도록 구현하면 될 것입니다. 우선 OpenGL ES 2.0을 지원하는 기기에서만 설치될 수 있도록 해야합니다.&amp;lt;uses-feature android:glEsVersion=&quot;0x00020000&quot; android:required=&quot;true&quot; /&amp;gt;AndroidManifest.xml에 위 코드를 포함하면 됩니다. 그렇다면 OpenGL ES 3.X의 지원여부는 어떻게 알 수 있을까요? 이 부분은 아래 EGL 초기화 과정에 포함이 되어있습니다.EGLSurfaceTexture는 이미지 스트림 생성자에서 받은 이미지를 Texture로 사용하여 새로운 이미지를 그려내야 합니다. 그렇다면 어디에 그려야 할까요? 이 앱에서는 화면을 보여주기 위한 SurfaceView와, 동영상을 저장하기 위한 MediaCodec 두 가지가 필요합니다. OpenGL ES로 렌더링한 결과물은 어디에 그려지는지와, OpenGL ES로 렌더링을 하기 전 초기화 과정에 대해 알기 위해서는 EGL에 대한 이해가 필요합니다.EGL은 크로노스 그룹의 API들과 여러 플랫폼의 호환을 위해 만들어졌습니다. OpenGL ES 뿐만이 아니라 OpenGL, OpenVG를 사용할 때도 쓰이며, 이러한 라이브러리들이 Android 외에도 다양한 OS와 프로그램에 쓰이기 때문에 이 둘을 이어주는 인터페이스가 필요했기 때문입니다.EGL의 초기화 단계를 차례대로 살펴보겠습니다. 첫 번째 단계는 EGLDisplay 설정입니다.EGLDisplayprivate fun getEglDisplay() { eglDisplay = EGL14.eglGetDisplay(EGL14.EGL_DEFAULT_DISPLAY) if (eglDisplay === EGL14.EGL_NO_DISPLAY) throw Exception(&quot;EGLDisplay 가져오기 실패&quot;)}EGL14는 EGL 1.4를 의미합니다. EGL 1.5가 Android 10(API 29)부터 지원되지만 미만의 버전에서 호환성이 우려되어 API 17에 추가된 EGL 1.4를 사용하였습니다.eglGetDisplay(displayId: Int) 함수를 통해 연결할 디스플레이를 지정하고, 하드웨어 사양을 가져옵니다. EGL14.EGL_DEFAULT_DISPLAY로 지정하면 OS에서 지정한 기본값을 넣어주게 됩니다.eglInitializeprivate fun eglInit() { val version = IntArray(2) if (!EGL14.eglInitialize(eglDisplay, version, 0, version, 0)) throw Exception(&quot;EGL 초기화 실패&quot;)}다음으로는 eglInitialize(dpy: EGLDisplay, major: IntArray, majorOffset: Int, minor: IntArray, minorOffset: Int) 함수를 사용하여 EGLDisplay 연결을 초기화합니다. 파라미터가 많아서 어렵게 느껴질 수도 있지만 여기서 중요한 것은 eglDisplay 하나입니다. 위에서 선언한 eglDisplay 값을 그대로 넣어주면 됩니다.version은 함수 실행 결과 후 version에 EGL 버전 값이 들어가게 됩니다. 하지만 이후에도 EGL 1.4를 사용할 것이기 때문에 이후에 version을 사용하지는 않을 것입니다. 실행 결과는 Boolean 값으로 반환되며 false 반환시 초기화 실패입니다.eglBindprivate fun eglBind() { if (!EGL14.eglBindAPI(EGL14.EGL_OPENGL_ES_API)) throw Exception(&quot;EGL 렌더링 API 설정 실패&quot;)}이후 eglBindAPI(api: Int) 함수에서 OpenGL ES를 사용할 것을 선언합니다.EGLConfigprivate fun getEglConfig(version: Int) { var renderableType = EGL14.EGL_OPENGL_ES2_BIT if (version &amp;gt;= 3) { renderableType = renderableType or EGLExt.EGL_OPENGL_ES3_BIT_KHR } val attribList = intArrayOf( EGL14.EGL_RED_SIZE, 8, EGL14.EGL_GREEN_SIZE, 8, EGL14.EGL_BLUE_SIZE, 8, EGL14.EGL_ALPHA_SIZE, 8, EGL14.EGL_RENDERABLE_TYPE, renderableType, EGL_RECORDABLE_ANDROID, 1, EGL14.EGL_NONE ) val configs = arrayOfNulls&amp;lt;EGLConfig&amp;gt;(1) val numConfigs = IntArray(1) if (!EGL14.eglChooseConfig(eglDisplay, attribList, 0, configs, 0, 1, numConfigs, 0)) { Log.d(TAG, &quot;$version EGLConfig 실패&quot;) } eglConfig = configs[0]}이제 EGLConfig를 가져와야 합니다. 다른 함수들과는 다른 특이한 작동방식을 쓰는데, 위의 코드에서 attribList에 사용하기 원하는 속성과 값을 Array에 순서대로 넣어줍니다. 렌더링 방식은 일반적인 RGBA_8888을 사용할 것입니다.EGL14.EGL_RENDERABLE_TYPE은 비스마스킹 방식으로 값을 지정해주어야 합니다. 각 값들이 1, 2, 4, 8 등으로 다른 비트 값을 가지고 있기 때문에 or 연산을 하면 됩니다. 위의 코드에서 EGL14.EGL_OPENGL_ES2_BIT은 4이고, EGLExt.EGL_OPENGL_ES3_BIT_KHR은 64입니다. OpenGL ES는 하위호환이 되기 때문에 OpenGL ES 2.0까지만 지원하는 기기에서는 or 연산으로 더하는 과정을 빼고, OpenGL ES 3.0을 지원한다면 두 값을 or 연산으로 더한 값을 사용하면 됩니다.위의 getEglConfig(version: Int) 함수에서 getEglConfig(3)을 먼저 시도하여 성공하면 두 버전 모두를 사용하는 EGLConfig를 반환받고, 실패하면 getEglConfig(2)로 OpenGL ES 2.0만을 사용하는 EGLConfig를 반환받도록 코드를 작성하였습니다.EGL_RECORDABLE_ANDROID는 별도의 상수를 지정하였습니다. 12610(0x3142) 값을 가지고 있습니다. 이 상수는 EGLExt.EGL_RECORDABLE_ANDROID로 이미 지정이 되어있긴 하지만, API 26에 추가되었기 때문에 Min API 21이었던 제 앱에서는 별도로 값을 입력하였습니다.attribList 배열의 마지막 EGL14.EGL_NONE 값은 배열의 종료를 나타내는 값입니다.함수 실행 성공시에 configs에 속성에 맞는 EGLConfig가 반환됩니다.이외에도 속성이 많기 때문에 크로노스 그룹의 eglChooseConfig 문서와 함께 사용할 수 있는 값들에 대해 Android의 EGL14 페이지를 함께 참고해 사용 가능한 속성들을 찾으시는 것을 추천드립니다. 만약 속성들 중 입력하지 않은 값이 있다면 eglChooseConfig 링크에 적혀있는 기본값이 적용됩니다.EGLContextprivate fun getEglContext() { getEglConfig(3) if (eglConfig != null) { val gl3Setup = intArrayOf(EGL14.EGL_CONTEXT_CLIENT_VERSION, 3, EGL14.EGL_NONE) val context = EGL14.eglCreateContext( eglDisplay, eglConfig, EGL14.EGL_NO_CONTEXT, gl3Setup, 0 ) if (EGL14.eglGetError() == EGL14.EGL_SUCCESS) { Log.d(TAG, &quot;GLES3 Config&quot;) eglContext = context glVersion = 3 } } else { getEglConfig(2) val gl2Setup = intArrayOf(EGL14.EGL_CONTEXT_CLIENT_VERSION, 2, EGL14.EGL_NONE) val context = EGL14.eglCreateContext( eglDisplay, eglConfig, EGL14.EGL_NO_CONTEXT, gl2Setup, 0 ) Log.d(TAG, &quot;GLES2 Config&quot;) eglContext = context glVersion = 2 }}마지막으로 EGLContext를 생성하여야 합니다. EGL14.eglCreateContext(dpy: EGLDisplay, config: EGLConfig, shareContext: EGLContext, attribList: IntArray, offset: Int) 함수로 생성하면 되는데, 앞에서 생성한 EGLDisplay와 EGLConfig가 들어가고, shareContext는 데이터를 공유할 또다른 EGLContext를 적는 파라미터입니다. 이 코드에서는 해당하지 않았기 때문에 EGL14.EGL_NO_CONTEXT 값을 지정하였습니다.그 다음으로 들어가는 gl3Setup, gl2Setup은 eglBindAPI(api: Int)로 앞에서 선언했던 OpenGL ES의 버전을 선언하는 배열입니다. EGLConfig에서 attribList를 선언한 방법과 같이 속성과 값을 순서대로 넣고, EGL14.EGL_NONE 으로 배열의 끝을 알리는 방식입니다. EGL14.EGL_CONTEXT_CLIENT_VERSION 속성의 값을 3으로 설정하여 OpenGL ES 3.0이 가능한지 확인하고, 성공시 그대로 EGLContext를 가져와 사용합니다. 실패시 OpenGL ES 2.0을 같은 방식으로 시도합니다.실제로 사용시에는 OpenGL ES 2.0의 코드 위주로 사용하고, OpenGL ES 3.0을 사용하였을 때 성능 향상이 있을 경우에만 버전 확인 후 사용하고, 그 경우에도 OpenGL ES 2.0의 코드만으로도 작성할 수 있는 방법을 함께 구현하여 버전에 따라 사용하도록 하였습니다.이렇게 해서 EGL 초기화 과정을 마쳤고, OpenGL ES의 사용 가능한 버전을 런타임으로 확인하여 설정할 수 있었습니다. 다음 포스팅에서는 OpenGL ES를 사용하여 렌더링 과정에서 어떻게 동영상의 각 프레임 이미지를 표시하기 위해 텍스처를 초기화하고 이미지를 텍스처로 사용하는 법에 대해 작성하도록 하겠습니다." }, { "title": "[Android] 동영상에 그림 그리기 (2) - Surface", "url": "/posts/video_memo_2/", "categories": "Android, boomerang", "tags": "Android, Surface, SurfaceTexture", "date": "2021-12-21 00:00:00 +0900", "snippet": "동영상에 그림을 그려넣기 위해서 동영상을 각 프레임으로 나누어 그린 후 사용자에게 보여주고, 재생하기로 하였습니다(지난 포스트 참고).그렇다면 어떻게 동영상을 프레임으로 나누어 그릴 수 있을까요?SurfaceTexture는 이미지 스트림에서 프레임을 캡처하여 OpenGL ES의 Texture로 저장합니다.이미지 스트림에 대한 설명은 AOSP 그래픽 개요 문서에서 적혀 있듯이, 이미지 스트림 생산자에서 생성한 그래픽 버퍼를 이미지 스트림 소비자로 옮깁니다. 그 과정에서 버퍼를 옮기기 위해 사용하는 클래스가 Surface입니다.SurfaceTexture를 Surface + Texture로 생각하고 이 둘을 먼저 이해하는 것이 기능의 이해에 도움이 될 것입니다.SurfaceSurface (AOSP - Surface)는 이미지 스트림 생산자와 소비자 사이에서 버퍼를 전달하도록 하는 역할을 합니다.Surface는 이미지 스트림 소비자에서 생성하여 이미지 스트림 생산자에 전달하여 그리게 합니다. 그림을 그릴 종이를 전해주고, 생산자가 그 곳에 그림을 그리면 다시 종이를 가져온다고 생각하면 됩니다. 종이는 소비자가 생산자에게 주었지만, 그림은 생산자가 소비자에게 준 것이죠.아래부터 실제 구현 방식을 정리하겠습니다. Surface의 생성자 중 하나를 예로 들어보겠습니다.생성자Surface(surfaceTexture: SurfaceTexture)생성자의 파라미터로 이미지 스트림 소비자 중 하나인 SurfaceTexture가 들어갑니다. 여기서 만들어진 Surface 인스턴스에 버퍼를 넣으면, SurfaceTexture에서 사용할 수 있게 됩니다. 그렇다면 이제 이미지 스트림 생성자를 찾아 Surface 인스턴스를 전달하면 될 것입니다.Surface를 가져오는 다른 방법Surface 클래스에서 직접 인스턴스를 생성하는 방법 외에도 Surface를 가져올 수 있는 방법이 있습니다. 몇몇 클래스에서 Surface를 가져올 수 있는 기능을 제공하는데, 바로 이 앱에서 사용자에게 동영상을 보여주는 SurfaceView와, 동영상을 저장하는 MediaCodec이 이에 해당합니다! 동영상에 그림을 그리는 기능은 전체에 걸쳐 Surface가 중요한 기능을 하는 것이죠.SurfaceViewSurfaceView는 화면에 보여지는 부분이므로 Button, TextView와 같은 다른 View처럼 레이아웃 파일에 뷰 계층구조의 일부로 포함되어야합니다. 하지만 Inflate 이후 바로 사용할 수 있는 다른 View들과 다르게 SurfaceView는 Inflate 이후 바로 사용할 수 없습니다. Surface가 생성된 이후에 사용을 해야 하는데, 이에 대한 콜백 인터페이스가 제공되기 때문에, 이를 사용하면 됩니다. 이에 대한 자세한 설명은 아래 SurfaceHolder에서 이어가겠습니다.MediaCodecMediaCodec도 인코딩할 동영상의 버퍼를 Surface로 받을 수 있습니다. createInputSurface() 함수를 사용하면 Surface를 반환합니다. 이 Surface에 렌더링을 할 때는 하드웨어 가속을 지원하는 API를 사용하는 것이 권장됩니다. 대표적으로 OpenGL ES가 있고, Canvas도 lockHardwareCanvas() 함수를 사용하여 반환받을 시에는 하드웨어 가속이 적용됩니다.SurfaceHolderSurface의 소유권을 앱과 공유하기 위한 인터페이스입니다. 콜백을 통해 Surface의 상태 변화를 수신할 수 있습니다.public interface Callback { void surfaceCreated(@NonNull SurfaceHolder var1); void surfaceChanged(@NonNull SurfaceHolder var1, int var2, int var3, int var4); void surfaceDestroyed(@NonNull SurfaceHolder var1);}SurfaceHolder.class의 코드 중 콜백에 해당하는 부분입니다. 액티비티나 프래그먼트에서 SurfaceHolder.Callback을 인터페이스 상속하여 구현하면 됩니다.콜백이 호출되는 순서는 일반적으로 아래와 같습니다.액티비티가 시작되면onCreate() -&amp;gt; onStart() -&amp;gt; onResume() -&amp;gt; surfaceCreated() -&amp;gt; surfaceChanged()의 순서로 호출이 됩니다. 또한 액티비티가 포그라운드에서 사라질 때onPause() -&amp;gt; surfaceDestroyed()의 순서로 호출이 됩니다.SurfaceView의 Surface를 가져오기 위해서는 SurfaceView에서 직접 가져오는 방식이 아닌, SurfaceHolder에서 가져오는 방식을 사용하여야 합니다.override fun surfaceCreated(p0: SurfaceHolder) { surface = p0.surface}이렇게 가져오면 됩니다.액티비티, 프래그먼트 생명 주기와 WeakReference인 Surface의 특성으로 인해 surfaceDestroyed()가 호출될 가능성이 항상 존재하기 때문에 Surface가 파괴되고 다시 생성될 가능성에 항상 대비하여야 합니다.Surface의 사용지난 포스트에서 소개해드린 구조를 다시 한 번 살펴봅시다.여기에서 화살표는 모두 Surface 클래스를 통해 이미지 버퍼가 전달되는 만큼, Surface 클래스는 기능의 구현에 있어 가장 핵심적인 요소입니다. 그렇다면 이제 화살표가 어떻게 연결되는지, 즉 이미지 스트림의 소비자와 생산자는 어떻게 연결되는지 알아보겠습니다.이미지 스트림 소비자와 생산자 연결MediaPlayer.setSurface(surface: Surface)동영상을 재생하는 MediaPlayer 클래스를 예로 들면, setSurface 함수에 위에서 생성한 Surface 인스턴스를 전달하면 됩니다.이제 MediaPlayer에서 동영상을 재생하면, Surface를 통해 SurfaceTexture로 버퍼가 넘어갈 것입니다. 이제는 프레임 단위로 나눌 차례입니다.SurfaceTextureonFrameAvailableListenersurfaceTexture.setOnFrameAvailableListener(this)override fun onFrameAvailable(p0: SurfaceTexture?) { // 새로운 Frame에 그림 그리기}방법은 의외로 간단합니다! SurfaceTexture가 새로운 프레임을 받을 때마다 호출되는 콜백 인터페이스를 가지고 있기 때문입니다. 콜백 인터페이스를 상속하고, 오버라이드하기만 하면 됩니다. 콜백 인터페이스의 대상이 되는 Fragment나 Activity를 setOnFrameAvailableListner()를 통해 전달하는 코드 한 줄만 더하면 됩니다.SurfaceTexture에 대한 내용은 이후 텍스처에 대한 설명과 함께 자세히 다루어보도록 하겠습니다.이제 새로운 프레임에 그림을 그려야 합니다. SurfaceTexture.updateTexImage()함수를 사용하여 새로 받은 프레임을 Texture로 사용할 수 있습니다. Texture는 무엇이고 그림은 어떻게 그릴 수 있는 것일까요? 이 내용부터는 OpenGL ES에 대한 설명이 동반 되어야 하므로 다음 포스트에 이어서 작성하도록 하겠습니다." }, { "title": "[Android] 동영상에 그림 그리기 (1) - 개요", "url": "/posts/video_memo_1/", "categories": "Android, boomerang", "tags": "Android", "date": "2021-12-20 00:00:00 +0900", "snippet": "boomerang이번 포스트부터는 부스트캠프 그룹프로젝트 기간에 개발한 미디어 메모 앱, 특히 제가 구현한 부분인 OpenGL ES 렌더링 방식의 동영상 메모 앱의 구현 과정과 구조에 대해 정리하려고 합니다.boomerang - 미디어 메모 앱데모 영상동영상 메모사용자에게 보여주기동영상 메모는 위와 같이 재생되고 있는 동영상 위에 사용자가 그림을 그리는 등 메모를 작성하면 그림이 그려진 동영상이 저장되고, 이를 공유할 수 있는 기능입니다.이 기능을 세가지로 나누어서 생각해보면 동영상을 보여주기 동영상에 그림을 그리기 그려진 동영상을 저장이렇게 나눌 수 있을 것입니다.그런데 이 방법을 순서대로 따라가면 한 가지 문제점이 있습니다. 동영상을 재생하면서 사용자에게 보여주면, 그려진 그림은 어떻게 보여줄 수 있을까요? 이미 원본 동영상은 재생되고 있고, 거기에 그림을 그려 저장한다면 그림이 그려진 동영상은 저장되지만 그 그림을 사용자가 바로 확인할 수가 없습니다.즉, 사용자는 자신이 어떤 그림을 그리는지 확인하지 못한다는 것입니다! 그리고 있는 그림을 저장이 되고 나서야 확인할 수 있다면, 아무래도 썩 좋은 사용자 경험은 아닐 것 같습니다.그래서 순서를 바꿔서 생각하였습니다. 동영상에 그림을 그리기 동영상을 보여주기 그려진 동영상을 저장이 방법에서 고민할 점은 어떤 동영상에 그림을 그려야 하는지 1번에서 어떻게 알 수 있을까요? 우선 그림을 그리기 전에 그릴 대상이 존재해야 할 것입니다. 그래서 처음에 과정을 하나 더하였습니다. 동영상을 재생하기 (사용자에게 보여지지는 않음!) 동영상에 그림을 그리기 동영상을 보여주기 그려진 동영상을 저장이렇게 한다면 우선 동영상을 재생하고, 그곳에 그림을 그린 후 그 결과를 사용자에게 보여주게 될 것입니다. 그리고 그 동영상을 저장합니다.동영상에 그림 그리기그렇다면 동영상에 그림을 어떻게 그릴 수 있을까요? 처음에 떠오른 두 가지 방법이 있습니다. 동영상을 재생시키고 그 위에 Canvas로 그림을 그리기 프레임 단위로 나누어서 각 프레임마다 그림을 그리고 다시 합치기첫 번째 방법이 더 찾기 쉬워보입니다. 하지만 그렇다면 어떻게 저장을 할 수 있을까요? 물론 MediaProjection 클래스를 사용하여 화면 전체를 녹화할 수 있습니다. 하지만 이 클래스는 SurfaceFlinger까지 거치고 난 뒤의 사용자에게 표시되는 화면 그 자체를 녹화해 상태바를 포함한 모든 UI가 녹화되는 기능입니다. 이렇게 한 뒤 동영상 부분만 잘라서도 저장할 수 있을 것 같지만, 더 좋은 방법이 있을 것 같아 두 번째 방법도 생각해보았습니다.두 번째 방법은 동영상을 프레임 단위로 나누어 하나씩 그림을 그리고 다시 합쳐서 동영상으로 만들어 저장하는 것입니다. 이 방법을 사용하는 것은 꽤 어려워 보였습니다. 자료가 많지 않아서 어떻게 접근해야 하는지조차 알기 어려웠습니다.저는 AOSP 그래픽 문서, Google에서 Android 프레임워크의 그래픽 관련 기능들을 구현한 프로젝트 Grafika에서 큰 도움을 얻을 수 있었습니다.자세한 내용은 이후 포스트에서 다루겠지만 간단히 정리하자면 이미지 스트림 생성자는 이미지 스트림 소비자에게 표시할 이미지의 버퍼를 Surface라는 클래스를 통해 전달할 수 있습니다.동영상을 재생하는 MediaPlayer도 이미지 스트림 생성자 중 하나입니다. 이미지 스트림 소비자는 일반적으로 화면에 표시될 UI들을 합성하는 SurfaceFlinger지만 우리는 바로 사용자에게 보여주지 않고 그림을 그린 뒤에 보여주기로 하였습니다. 그렇다면 다른 이미지 스트림 소비자에 연결되어야 할 것입니다.OpenGL ES를 사용하는 클래스들도 이미지 스트림 소비자가 될 수 있습니다. OpenGL ES는 그래픽 렌더링 라이브러리로, Canvas와 같은 다른 안드로이드 프레임워크와 다르게 하드웨어 가속 렌더링을 수행할 수 있어 성능에 큰 장점을 가지고 있습니다.그래서 MediaPlayer를 OpenGL ES 렌더링을 수행할 수 있는 SurfaceTexture 클래스와 연결시켰습니다. 그리고 SurfaceTexture에서는 프레임에 그림을 더하고, 그 결과를 화면에 표시하고 다시 동영상으로 저장할 수 있을 것입니다.정리위 내용들을 간단히 정리하면 위 그림처럼 나타낼 수 있습니다. 위에서 언급하지 못한 내용이 많지만 내용이 너무 길어져 하나씩 따로 작성하겠습니다.여기까지가 동영상 메모의 기본적인 아이디어였습니다. 다음 포스트부터는 각 과정에 대해 자세히 다뤄보도록 하겠습니다." }, { "title": "부스트캠프 웹·모바일 6기 수료 후기", "url": "/posts/intro/", "categories": "boostcamp", "tags": "boostcamp", "date": "2021-12-09 00:00:00 +0900", "snippet": "수료한여름에 시작했던 부스트캠프(모바일 Android)를 지난주 금요일에 마쳤습니다. 5개월에 가까운 긴 시간이었지만 돌이켜보니 순식간이네요. 아무래도 부스트캠프 외에 다른 일들에 신경 쓰기에는 너무 벅차서 지난 시간 동안 다른 것들이 제 삶에 없었기 때문이라고 생각합니다.부스트캠프 수료 후 해야겠다고 생각한 것이 크게 3가지가 있는데, 코딩 테스트 준비 배운 내용을 복습할 수 있는 토이 프로젝트 하나 진행하기 블로그 만들어서 구현한 프로젝트 정리하기입니다. 부스트캠프의 컨텐츠를 유출하는 것은 금지되어 있기 때문에, 제가 블로그에 작성할 수 있는 내용은 부스트캠프 멤버십에서 마지막으로 진행했던 유일한 Public Repository인 그룹프로젝트에 대한 것만 가능합니다.지난 5개월간 배운 내용을 이 프로젝트 하나로 표현을 해야한다는 생각을 가져서 주제 선정부터 설계, 구현, 테스팅까지 고민을 많이 했는데 이 때의 과정을 까먹기 전에 정리해야겠다는 생각으로 작성하려 합니다.그 전에 이 글에서 부스트캠프를 되돌아보려고 합니다.부스트캠프 챌린지미션 내용을 유출하는 것이 금지되어 있어 공개된 내용을 위주로 적겠습니다. 부스트캠프 웹·모바일 페이지에서 소개하는 것처럼 매일 주어지는 미션을 수행해야 합니다. 그리고 적혀있는대로 강도가 생각보다 높았습니다.또한 챌린지의 성과를 토대로 이후 멤버십 합격 여부를 결정하다보니 미션을 그냥 포기하거나 타협할 수도 없었고, 매일 미션을 수행하므로 미룰 수도 없었습니다. 올 여름은 유난히 더웠던 것으로 기억나는데 그래서인지 정말 힘들었던 기억이었습니다.시작하기 전에 최대한 많이 공부할수록 좋지만, 적어도 사용해야 할 언어에 대해서는 미리 공부를 한 후에 참가하시는 것을 강력하게 권고드립니다.부스트캠프 멤버십그룹프로젝트부스트캠프의 마지막 6주는 팀을 짜서 설계부터 테스팅까지 직접 앱을 만들어야 합니다. 앞에서 배웠던 내용들도 중요하지만 그 내용들은 부스트캠프의 미션으로 만들어진 앱이기 때문에 밖으로 유출할 수 없어서 직접적으로 남길 수 있는 결과물은 이 그룹프로젝트를 통해 만들어진 앱이 유일하고, 그래서 중요한 기간입니다.이 부분에서 저는 승부수를 던졌습니다. 제가 이전까지 공부해 본 적이 없고, 다른 사람들도 잘 선택하지 않는 주제를 다루기로 결정하였습니다.동영상에 그림을 그리자는 아이디어가 떠올랐는데, 팀원들이 처음에는 부정적인 반응이라서 포기하고 네이버 메모 클론 앱을 만드는 쪽으로 의견이 모였습니다. 하지만 팀원분들의 생각에 변화가 있었는지 주제 선정 마감 전 날에 제 의견을 채택하여서 빠르게 방향을 수정하였습니다.아이디어를 구체화시켜보니 단순히 그림을 그리는 것에 끝나지 않고, 당연하게도 그것을 저장할 필요가 있었습니다. 그리고 난 뒤에 다시 볼 수 없다면 그런 기능이 있을 필요가 없을 것입니다. 필요한 기능의 리스트를 정리해보자면 동영상을 재생 재생 중에 그림을 그리기 동영상과 그려진 그림을 합해 저장이 기능들 중 단 하나도 이전에 구현해 본 적이 없어서 어떻게 해야할지 감이 잡히지 않았지만 부스트캠프에서 짧은 시간에 학습하고 구현하는 것에 익숙해진 것인지 이전보다는 빠르게 구현을 위해 학습해야 할 내용들을 파악할 수 있었습니다.그리고 새로운 도메인을 배우고 기능을 구현할 때마다 성취감과 재미를 이전보다 더 크게 느낄 수 있어서 의욕이 더 컸습니다. 제가 하고 싶었던 주제를 직접 개발하기도 했고, 제가 선정한 주제기 때문에 책임감이 크게 느껴졌기 때문이기도 합니다.제가 개발을 시작한 이후로 가장 재밌게 개발을 했다고 생각됩니다.운이 좋게도 적극적으로 피드백과 의견을 제시해주는 팀원들을 만나 의견을 듣고, 저도 제 의견을 이전보다 더 적극적으로 제시하면서 프로젝트의 방향을 결정하는 등 다양하고 많은 대화를 통해 신속한 결정을 내릴 수 있었습니다.구현한 내용들에 대해서는 이 블로그에 작성할 예정입니다.추천?부스트캠프는 직접적으로 가르쳐주는 내용은 많지 않았고, 특히 A부터 Z까지 알려주는 것은 일정 외에는 없다고 보셔도 됩니다. 다만 스스로 찾아서 배울 수 있도록 미션의 설계가 잘 되어 있다고 느꼈습니다.저는 부스트캠프 이전에 혼자서 공부를 할 때 어떤 것을 공부해야 하는지 알기 어려워 앱을 만들면서 필요했던 것을 그 때마다 찾아가며 배웠습니다.이러한 방식이 안드로이드에 대해 처음 배울 때에는 큰 도움이 되었지만 왜 필요한지, 왜 여기서 이러한 코드여야 하는지에 대해서는 알기 어려웠습니다. 필요할 때 찾아서 적용하고 그만이다보니 지식이 파편화 되어있던 상태라고 생각합니다.그리고 동료들과 피드백 시간이 항상 존재하기 때문에 배웠다고 생각하는 것에 대해 정리하여 말하면서 다시 확인하고, 동료들의 의견을 들으면서 점검할 수 있었습니다. 이 과정에서 저는 크게 성장할 수 있었다고 생각합니다.정리하자면 저처럼 혼자 공부하다가 더 이상 발전이 되지 않고 있다는 생각이 들거나 다른 분들과 피드백을 주고 받으며 성장하고 싶다면 추천합니다. 저에게 있어서 이 경험은 굉장히 큰 도움이 되었습니다." } ]
